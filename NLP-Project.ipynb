{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Items from Yelp reviews\n",
    "###### Ashwin Sura Ravi | Dileep Kumar Gunda | Suryakiran Suresh Gurumoorthy Iyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install following packages!\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install inflect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Gather Data\n",
    "#### Data used in the code is located at, https://drive.google.com/open?id=1C4laG6BFKFyiPjy27dRyBu6bG-uUzgBT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Yelp Dataset\n",
    "\n",
    "Filter Yelp Dataset for restaurants(category) with number of reviews between 100 and 150. <br> <br>\n",
    "*INPUT:* <br>\n",
    "**yelp_academic_dataset_business.json**: Meta Information(name, ratings, #reviews, etc) about all business in Yelp including restaurants <br>\n",
    "**yelp_academic_dataset_review.json**: Contains reviews corresponding to all businesses present in above file<br><br>\n",
    "*OUTPUT:* <br>\n",
    "**restaurants.json**: list of restaurants sayisfying given condition <br>\n",
    "**reviews.json**: contains reviews corresponding to all restaurants in restaurants.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "review_count_threshold = 100\n",
    "restaurant_ids = []\n",
    "restaurant_file = 'data/restaurants.json'\n",
    "review_file = 'data/reviews.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total restaurants: 3338, total reviews: 408196, avg reviews: 122\n"
     ]
    }
   ],
   "source": [
    "# extract restaurants\n",
    "\n",
    "restaurant_writer = open(restaurant_file, 'w')\n",
    "total_reviews = 0\n",
    "restaurant_ids = []\n",
    "with open('data/yelp_academic_dataset_business.json') as json_file:\n",
    "    for line in json_file:\n",
    "        json_line = json.loads(line)\n",
    "        #print \"business: %s, review count: %s\" % (json_line['name'], json_line['review_count'])\n",
    "        categories = json_line['categories']\n",
    "        if categories != None:\n",
    "            categories = categories.lower()\n",
    "        if json_line['review_count'] > review_count_threshold and json_line['review_count'] < 150 and 'restaurant' in categories:\n",
    "            restaurant_ids.append(json_line['business_id'])\n",
    "            total_reviews += json_line['review_count']\n",
    "            restuarant = {}\n",
    "            restuarant['business_id'] = json_line['business_id']\n",
    "            restuarant['name'] = json_line['name']\n",
    "            restuarant['stars'] = json_line['stars']\n",
    "            restaurant_writer.write('%s\\n' % (json.dumps(restuarant)))\n",
    "\n",
    "restaurant_writer.close()\n",
    "print 'total restaurants: %s, total reviews: %s, avg reviews: %d' % (len(restaurant_ids), total_reviews, total_reviews/len(restaurant_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reviews written, 618060\n"
     ]
    }
   ],
   "source": [
    "# extract reviews for restaurants\n",
    "\n",
    "review_writer = open(review_file, 'w')\n",
    "total_reviews = 0\n",
    "with open('data/yelp_academic_dataset_review.json') as json_file:\n",
    "    for line in json_file:\n",
    "        json_line = json.loads(line)\n",
    "        if json_line['business_id'] in restaurant_ids:\n",
    "            del json_line['user_id']\n",
    "            del json_line['funny']\n",
    "            del json_line['cool']\n",
    "            review_writer.write('%s\\n' % (json.dumps(json_line)))\n",
    "            total_reviews += 1\n",
    "\n",
    "review_writer.close()\n",
    "print 'total reviews written, %s' % (total_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Generating Labels and Truth values (Training and Test set)\n",
    "\n",
    "Computing popular items, require identifying food entities in each review and sentiment of each entity. Yelp dataset doesn't contain such data nor the menu for each restaurant. So below we selected 7 restaurants and annotated every review for these 7 restaurants with food entities and the user sentiment for each entity. <br>\n",
    "Below are the 7 restaurants names, there exists 7 JSON files with same name, each containing reviews for that corresponding restaurant. <br><br>\n",
    "9UTpmQ4OhX5jNFUIu7dPPQ <br>\n",
    "dusNIzdCaH6EoLl2hRy6cQ <br>\n",
    "FDEm-c3NAXnTVtl-hgzAhA <br>\n",
    "Hdzo5ggPswyv-8ZlW0PVLw <br>\n",
    "ILA41PhErg4bl1royzDX4g <br>\n",
    "L0aSDVHNXCl6sY4cfZQ-5Q <br>\n",
    "RGK23CEkDfYHWtUbRhA-bQ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 7 restaurants and extracting reviews of them\n",
    "import json\n",
    "\n",
    "restaurant_file = 'data/restaurants.json'\n",
    "selected_restaurants = []\n",
    "with open(restaurant_file, 'r') as json_file:\n",
    "    for line in json_file:\n",
    "        json_line = json.loads(line)\n",
    "        if not json_line['business_id'].startswith('-'):\n",
    "            selected_restaurants.append(json_line['business_id'])\n",
    "            if len(selected_restaurants) == 7:\n",
    "                break\n",
    "\n",
    "# print(selected_restaurants)\n",
    "\n",
    "# extracting reviews for those selected restaurants\n",
    "review_file = 'data/reviews.json'\n",
    "for res_id in selected_restaurants:\n",
    "    res_writer = open('data/tmp/'+res_id + '.json', 'w')\n",
    "    with open(review_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            json_line = json.loads(line)\n",
    "            if json_line['business_id'] == res_id:\n",
    "                res_writer.write('%s\\n' % (json.dumps(json_line)))\n",
    "    res_writer.close()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Tool\n",
    "Tool to annotate each of the above files with entity and its corresponding sentiment. It writer annotation to file names **tag_{restaurant-id}.json**. Later all these files, reviews and annotations are merged into single files named **reviews.json** and **review_tags.json**. <br><br>\n",
    "*Hereafter reviews.json file corresponds to reviews of the above selected 7 restaurants.They are inside **tmp** folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REVIEW: #21\n",
      "This place is an oasis in the busy FCP food court.they have their own enclosed seating area. They have excellent quiches, salads and baked goods. Try their chocolate chip cookies. Full service espresso bar and patio in the summer.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "quiches\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "salads\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "chocolate chip cookies\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #22\n",
      "Honestly, wow. I love this place - the decor, food, coffee, friendly staff all come together beautifully. What an oasis in the financial district.\n",
      "\n",
      "Absolutely try their baked goods. \n",
      "\n",
      "Maman serves up daily fruit tarts (usually later in the afternoon) beautifully made with flavours melding together perfectly. They're also not too sweet, have lots of fresh fruit and the presentation's on-point (peach blueberry almond tart, apple tart)! Usually the pound cakes are served up as samples. I prefer the orange cake and matcha ones - they're moist and flavourful. Oh, and Maman's chocochip cookie is anything but average: chunky, nutty (macadamias, almonds, and walnuts), with the right amount of chocolate chips - a worthy splurge (~$3.50) to satisfy that mid-afternoon sweet tooth ;) \n",
      "\n",
      "On the subject of savoury food: if you're deciding which quiche to try, get the Ham and Cheese. Flavourful fluffiness abounds! You can make it a combo with one of Maman's salad (~$11.50). \n",
      "\n",
      "Finally, bevvies: Lavender hot choco is delightful. Drip coffee is pretty dece! And their lattes are smooth though, I prefer Mos Mos. However, their coffee station is the cutest - well stocked utensil drawer and lovely containers for your choice between cream/milk/soymilk!\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "fruit tarts\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "orange cake\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "matcha\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "chocochip cookie\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "quiche ham and cheese\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "lavender hot chocolate\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "lavender hot choco\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #23\n",
      "VERY VERY slow service. Went around 8:30AM. Only 2 people in front who ordered coffee (americano and cappuccino). Took almost 10 minutes to take their orders. Finally received double espresso. The coffee was very sour (some like that flavour profile, I don't) and gritty (beans too finely ground). I won't be going back.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "double espresso\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #24\n",
      "Beautiful interior. Good Americano. Delicious desserts (ordered the lavender loaf, apple pie, and yogurt parfait). A hidden gem in a typical modern food court.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "lavender loaf\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "apple pie\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "yogurt parfait\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #25\n",
      "*This review only reflects the brunch service. \n",
      "*For their brunch, I really appreciated how Maman don't have the typical egg benedict that you would see on every brunch's menu in Toronto.\n",
      "*You can make a reservation for their weekend brunch through email. \n",
      "*The staff are true Parisian, no Quebecois french (for those who cares....) \n",
      "*The service was amazing and tentative. \n",
      "*Quite and relaxing ambiance. \n",
      "\n",
      "-Matcha Latte - $5: \n",
      "The drink quality here are always consistent! Instead of normal milk, Maman servers the matcha latte with almond milk. I liked how the latte wasn't too sweet and you can taste the strong (almost grassy) taste of matcha, which is how I like it :).\n",
      "\n",
      "-Smoke Salmon served on blinies bread with yogurt sauce, lemon, dill & espelette pepper - $15: \n",
      "Friend loved it and licked the plate clean. \n",
      "\n",
      "-Croque \"maman\" with parisian ham, comté cheese and béchamel sauce (optional with a sunny side-up egg +$2) - $15: \n",
      "The croque madame, or \"maman\" came out piping hot and it was a huge portion! Without the egg, the sandwich would still have been really filling and it was worth the $13 price tag. The side salad had a bit too much dressings, but it was refreshing after having a heavy sandwich.\n",
      "\n",
      "Oh boy, I normally dislike the béchamel sauce as some tends to taste a bit too flour-y or bland, but this was incredibly flavorful and the sauce was not too thin nor thick. The cheese just slightly melted, creamy and buttery. Break the egg yolk and it will runs through your sandwich and mixes with the cheese and béchamel sauce, and it turns into this eating experience that is just beyond words. \n",
      "\n",
      "-Matcha green tea pancakes with blueberries, ricotta & shredded coconut - $14: \n",
      "The matcha flavor didn't come through and it was almost undetectable.I couldn't taste the ricotta nor shredded coconut either. The texture of the pancake was too doughy, dense and almost rubbery. I thought the portion was quite small for $14, but it was a good sized portion without making you feeling stuffed. The one positive thing I can think of was that the pancake was not too sweet, even with the syrup and whipped cream.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "egg benedict\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "matcha latte\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "smoke salmon\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "croque\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "matcha green tea pancakes\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #26\n",
      "Visiting Maman during my work day for a Jasmine Mist tea is like a little visit to the French country side. I absolutely love how cute it is in there and the atmosphere brightens my day.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "jasmine mist tea\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #27\n",
      "This place is quaint and serves a decent selection of pastries, coffee and tea. \n",
      "\n",
      "The only con is their lunch selection could be so much better. Limited selection and for the price, I could get a better lunch at IQ or Kupfert and Kim.\n",
      "\n",
      "Most likely won't be back as there are better lunch and coffee options in the PATH.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "pastries\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "coffee\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "tea\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #28\n",
      "Tried the Lavender Hot chocolate. It was okay. No overwhelming Lavender taste. The ambience is cute. And oh.......they do not do brunch anymore.\n",
      "\n",
      "June 30, 2017\n",
      "I had the Cappucino and the Blueberry bread. Both are expensive but the Blueberry loaf was really flavourful.  I tasted the Pistachio as well. I prefer the Blueberry.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "lavender hot chocolate\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "blueberry load\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "blueberry loaf\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "pistachio cake \n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #29\n",
      "Cutest cafe in the upstairs food court at First Markham Place. Everything in here is absolutely Pinterest-worthy, from the food displays to the decor to the cup designs. It comes at a price, but once in a while, everyone deserves a fancy cup of Joe. \n",
      "\n",
      "They have a lot of pastries, but the pistachio loaf is really good! I love it as a afternoon break from work and a special treat to myself.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "pistachio loaf\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #30\n",
      "I've been here several times! The hot chocolate is super delicious (and super hot, be careful). The matcha latte is smooth and made with almond milk. \n",
      "\n",
      "I expected more from the pain au chocolat, and I think I've had better chocolate croissants at other places, like Duo. Even Starbucks' chocolate croissant served warm seemed better.\n",
      "\n",
      "Specialty drinks come in one size only.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "hot chocolate\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "matcha latte\n",
      "sentiment: \n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next entity: (<entity_name>/n)\n",
      "chocolate croissants\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #31\n",
      "This Parisian style bakery/cafe place is super cute. It reminds me of a little cafe you can find in the southern region of France. This is the second location of the Maman chain. The first cafe is in the SoHo neighbourhood in Manhattan. The cafe has a rustic feel to it and it also feels very cozy. Like other Yelpers say, this cafe is actually difficult to find. It's in the upstairs food court in First Canadian Place near the IQ restaurant. Be sure to come earlier during lunch to avoid the lunch rush. The line up gets hectic around here since it is located in the financial district. \n",
      "\n",
      "Went during lunch and my party only ordered the \"two salads for $11\" option. Okay hands down, their salads are pretty good especially the one that has cranberries (I believe the salad was called Julie). Though the salads are pricey, I think it is worth trying it at least once in your life time. Also, their menus change frequently so be sure to check their website to see what they are serving on that specific day. I also ordered their ice tea ($3). Their ice tea is what drove down this rating to a 4. The ice tea was not good... highly do not recommend. I haven't tried their other baked goods or coffee/lattes but I'm sure they would be decent and better than the ice tea. Their packaging is also super cute and trendy.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "salads\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "ice tea\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #32\n",
      "This is the cutest French cafe! I always come for their salads, Sloan tea and cake loafs (so moist and gooey omg) The food quality is great but like other reviewers have said, the price is a bit steep and it wouldn't be something I would have every day...more like afternoon pick me ups or end of the week treats. They also carry my favourite ginger ale. Overall I would recommend it if you haven't tried!\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "salads\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "loafs\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "ginger ale\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #33\n",
      "I've been here 3 times since it opened. At first I was extremely mesmerized by the decor - Maman is beautifully decorated with long wooden tables, flowers and vintage furniture. I fell in love with it. \n",
      "\n",
      "However since visiting a few more times, I have become quite disappointed with the food/drink quality. The coffee is not the best, a little too watery for me. I've tried the proscuitto sandwich at lunch which was the most disappointing of all - the bread tasted cheap, and there were barely any ingredients inside. In the sea of mediocre food court options, this sandwich managed to be fly under my expectations. \n",
      "\n",
      "In terms of baked goods, the lavender cake is average - it's unique and flavourful, but it tasted too heavy/oily so I was not able to finish it. The best pastry on the menu is definitely the nutty chocolate chip cookie. The cookie is warmed up, soft on the inside with tasty chocolate chips! \n",
      "\n",
      "Hopefully their other lunch menus are better because Maman is too cute to have disappointing menu options.\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "coffee\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "proscuitto sandwich\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "chocolate chip cookie\n",
      "sentiment: \n",
      "1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "y\n",
      "\n",
      "REVIEW: #34\n",
      "I really wanted to come to this place for brunch and so I did today! It's a tiny and cute little shop at the end of the food court area on the upper level. \n",
      "\n",
      "I wasn't fond of their ordering system since there weren't any signs indicating where the cashier was or where you are to order your goodies. I guess it was just my issue since most were regulars so they knew where to go... Anyways I ordered a matcha latte and their Croque Maman for a total of just under $15. This was steep for a sandwich and latte! \n",
      "\n",
      "I was given the Croque Maman right away in a takeout try even though I was going to be eating it there. When I had my first bite, it was very cheesy but I found that it wasn't hot. It was comparable to microwave food since my bread reminded me of a near stale texture. The overall flavours were so so but extremely salty. For the price you would expect more but to me this sandwich was overpriced. I guess the other patrons didn't find it as expensive since they were either bankers or investors as 90% were all in suits! I wouldn't order it again.\n",
      "\n",
      "The matcha latte was ready a few minutes later. I picked it up and with lattes you expect some cute design but there aren't any. I guess I had high expectations since I noticed another latte with a leaf design so it's only natural to assume yours would have one as well but NOPE! I didn't enjoy my latte either as it was just a tad bit strong in the matcha flavour that it left you with a really bitter aftertaste after each sip! There wasn't a right balance of matcha to latte taste. Since it was expensive, I tried to finish it but I couldn't. I ended up throwing it out since the bitterness was stronger near the end. \n",
      "\n",
      "Overall, this place wasn't what I expected it to be. I really liked this place based on the social media pictures but I guess that's all they ever will be... Not worth your money! You can get better food elsewhere and for cheaper!\n",
      "\n",
      "next entity: (<entity_name>/n)\n",
      "croque maman\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "matcha latte\n",
      "sentiment: \n",
      "-1\n",
      "next entity: (<entity_name>/n)\n",
      "n\n",
      "continue to next review: (y/n)\n",
      "n\n",
      "continue next time from record number: 35\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "review_file = 'data/tmp/h_ypoQ2rmwX8UGSmohsGiQ.json'\n",
    "annotate_file = 'data/tmp/tag_h_ypoQ2rmwX8UGSmohsGiQ.json'\n",
    "annotate_writer = open(annotate_file, 'a+')\n",
    "with open(review_file, 'r') as json_file:\n",
    "    i = 0\n",
    "    for line in json_file:\n",
    "        if i >= ptr:\n",
    "            json_line = json.loads(line)\n",
    "            annotation = {}\n",
    "            annotation['review_id'] = json_line['review_id']\n",
    "            annotation['entities'] = []\n",
    "            try:\n",
    "                print('\\nREVIEW: #%d\\n%s\\n' % (i, json_line['text']))\n",
    "\n",
    "                print('next entity: (<entity_name>/n)')\n",
    "                entity = raw_input()\n",
    "                while entity != 'n':\n",
    "                    print('sentiment: ')\n",
    "                    sentiment = int(raw_input())\n",
    "                    annotation['entities'].append({'entity': entity, 'sentiment': sentiment})\n",
    "                    print('next entity: (<entity_name>/n)')\n",
    "                    entity = raw_input()\n",
    "                annotate_writer.write('%s\\n' % (json.dumps(annotation)))\n",
    "                i = i+1\n",
    "                print('continue to next review: (y/n)')\n",
    "                y = raw_input()\n",
    "                if y == 'n':\n",
    "                    ptr = i\n",
    "                    annotate_writer.close()\n",
    "                    print(\"continue next time from record number: %s\" % (ptr))\n",
    "                    break\n",
    "            except ValueError as e:\n",
    "                annotate_writer.close()\n",
    "                ptr = i\n",
    "                print(e)\n",
    "                sys.exit(1)\n",
    "        else:\n",
    "            i = i+1\n",
    "annotate_writer.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Extracting (food) Entities from reviews\n",
    "### Preparing Generic Menu of restaurants\n",
    "We need restaurants menu to extract food entities from reviews. We couldnt find exact menu, but found yelp older dataset which contains menu of over 1000 restaurants. Menu is captured in file **yelp-old-data-restaurants.json**. Only menu is extracted into file **yelp-menu.txt** and its processed to refine the menu like removing special characters, splitting multiple items into individual lines. The processed and refined menu is captured in **proc_menu.txt**. <br><br>\n",
    "Over the time, *proc_menu.txt* has been refined directly using regular expressions in the file-editor. *Re-running the block will override the proc_menu.txt and the changes will be lost.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dkgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dkgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dkgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dkgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('had', 'VBD'), ('grilled', 'VBN'), ('chicken', 'NNS'), ('and', 'CC'), ('highly', 'RB'), ('recommend', 'VB'), ('it', 'PRP')]\n",
      "[('I', 'PRP'), ('had', 'VBD'), ('beef', 'NN'), ('pork', 'NN'), ('pizza', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"I had grilled chicken and highly recommend it\")\n",
    "print nltk.pos_tag(tokens)\n",
    "tokens = nltk.word_tokenize(\"I had beef pork pizza\")\n",
    "print nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare old yelp menu\n",
    "\n",
    "# unwanted_chars_re = '[\\'\\\"\\-0-9\\\\\\\\]'\n",
    "# dish_file = 'data/yelp-old-data-restaurants.json'\n",
    "# dish_writer = open('data/yelp-menu.txt', 'w')\n",
    "# dish_names = []\n",
    "# with open(dish_file) as json_file:\n",
    "#     for line in json_file:\n",
    "#         json_line = json.loads(line)\n",
    "#         if len(json_line['items']) > 0:\n",
    "#             for item in json_line['items']:\n",
    "#                 dish_names.append(re.sub(unwanted_chars_re, '', item['name'].lower()))\n",
    "#                 dish_writer.write(re.sub(unwanted_chars_re, '', item['name'].lower())+'\\n')\n",
    "\n",
    "# dish_writer.close()\n",
    "# print 'Total Dish names: %d' % (len(dish_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# process yelp menu\n",
    "\n",
    "# import re\n",
    "\n",
    "# yelp_menu_file = 'data/yelp-menu.txt'\n",
    "# processed_menu_writer = open('data/proc_menu1.txt', 'w')\n",
    "# yelp_menu = []\n",
    "# unwanted_chars_re = '[\\'\\\"\\-0-9]'\n",
    "# with open(yelp_menu_file, 'r') as txt_file:\n",
    "#     for line in txt_file:\n",
    "#         items = line.split('/')\n",
    "#         for i in items:\n",
    "#             i = re.sub(unwanted_chars_re, '', i.lower().strip())\n",
    "#             i = re.sub('[\\,\\(\\)\\+]', ' ', i)\n",
    "#             yelp_menu.append(i)\n",
    "#             processed_menu_writer.write(i+'\\n')\n",
    "# processed_menu_writer.close()\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting entities from reviews\n",
    "Extracting food entities for each review in reviews.json file. Its a general observation that any menu has a noun in it like *chicken burger*. The code uses this fact to detect nouns in each review, considers moving window, upto length 4 around that noun and then checks if that phrase is present in processed yelp menu. All such detected food entities are written to file below. <br><br>\n",
    "*OUTPUT*<br>\n",
    "**predicted_tags.json**: contains model extracted (food) entities for each review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funtions to transform and compare two menu items\n",
    "from nltk.stem import PorterStemmer\n",
    "import inflect\n",
    "\n",
    "inf = inflect.engine()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def normalize_word(word):\n",
    "    return word\n",
    "#     return porter.stem(word)\n",
    "\n",
    "def compare_words(word1, word2):\n",
    "    word1_pl = inf.plural(word1)\n",
    "    word2_pl = inf.plural(word2)\n",
    "    if word1 == word2 or word1 == word2_pl or word1_pl == word2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to search given phrase in processed yelp menu\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# read processed yelp menu\n",
    "# for now reading from variable yelp_menu\n",
    "def read_yelp_menu():\n",
    "    yelp_menu = []\n",
    "    with open('data/proc_menu.txt', 'r') as txt_file:\n",
    "        for line in txt_file:\n",
    "            yelp_menu.append(line.lower().strip())\n",
    "    return yelp_menu\n",
    "    \n",
    "def search_menu(item, yelp_menu):\n",
    "    # discard single character item search\n",
    "    if len(item) == 1:\n",
    "        return False\n",
    "    unwanted_chars_re = '[\\'\\\"\\-0-9]'\n",
    "    item = re.sub(unwanted_chars_re, '', item.lower().strip())\n",
    "    item = item.split()\n",
    "    if len(item) == 1:\n",
    "        # search only pefix of yelp menu\n",
    "        for yelp_item in yelp_menu:\n",
    "            if len(yelp_item.strip().split()) > 0:\n",
    "                # first word of yelp_item\n",
    "                y_word = normalize_word(yelp_item.strip().split()[0])\n",
    "                i_word = normalize_word(item[0])\n",
    "                if y_word == i_word:\n",
    "                    return True\n",
    "    elif len(item) > 1:\n",
    "        # search if all items are present in single menu instance of yelp menu\n",
    "        match = False\n",
    "        for yelp_item in yelp_menu:\n",
    "            match = True\n",
    "            for w in item:\n",
    "                w = normalize_word(w)\n",
    "                if match:\n",
    "                    match = False\n",
    "                    for y in yelp_item.split():\n",
    "                        y = normalize_word(y)\n",
    "                        if y == w:\n",
    "                            match = True\n",
    "            if match:\n",
    "                return True\n",
    "        return match\n",
    "    return False\n",
    "\n",
    "def segment_review(review, yelp_menu):\n",
    "    review_tokens = nltk.word_tokenize(review)\n",
    "    token_pos = nltk.pos_tag(review_tokens)\n",
    "    i = 0\n",
    "    items = set()\n",
    "    while i < len(review_tokens):\n",
    "        if token_pos[i][1].startswith(\"NN\"):\n",
    "            print(review_tokens[i]+\":\")\n",
    "            included = False\n",
    "            # previous word\n",
    "            if i-1 >= 0:\n",
    "                item = \" \".join(review_tokens[i-1:i+1])\n",
    "                if token_pos[i-1][1].startswith(\"VB\") or token_pos[i-1][1].startswith(\"JJ\"):\n",
    "                    if search_menu(item, yelp_menu):\n",
    "                        items.add(item)\n",
    "                        included = True\n",
    "            # next word     \n",
    "            if i+1 < len(review_tokens) and not included:\n",
    "                item = \" \".join(review_tokens[i:i+2])\n",
    "                if token_pos[i+1][1].startswith(\"VB\") or token_pos[i+1][1].startswith(\"JJ\"):\n",
    "                    if search_menu(item, yelp_menu):\n",
    "                        items.add(item)\n",
    "                        included = True\n",
    "            # only current word\n",
    "        i = i+1\n",
    "    print 'items: %s' % (items)\n",
    "    \n",
    "def segment_review_len(review, yelp_menu):\n",
    "    review_tokens = nltk.word_tokenize(review)\n",
    "    token_pos = nltk.pos_tag(review_tokens)\n",
    "    i=0\n",
    "    items = set()\n",
    "    included = []\n",
    "    while i < len(review_tokens):\n",
    "        if token_pos[i][1].startswith(\"NN\"):\n",
    "            found = False\n",
    "            for l in range(4, 0, -1):\n",
    "                if not found:\n",
    "                    for x in range(i-l+1, i+1):\n",
    "                        y = x+l\n",
    "                        if x >= 0 and y <= len(review_tokens):\n",
    "                            item = \" \".join(review_tokens[x:y])\n",
    "                            if len(included)==0 or included[-1][1]<x:\n",
    "                                if token_pos[x][1].startswith(\"VB\") or token_pos[x][1].startswith(\"JJ\") or token_pos[x][1].startswith(\"RB\") or token_pos[x][1].startswith(\"NN\"):\n",
    "                                    if search_menu(item, yelp_menu):\n",
    "                                        if len(included)>0 and x == 1+included[-1][1]:\n",
    "                                            included[-1] = (included[-1][0], y-1)\n",
    "                                        else:\n",
    "                                            included.append((x,y-1))\n",
    "                                        found=True\n",
    "                                        break\n",
    "        i+=1\n",
    "    for x,y in included:\n",
    "        items.add(\" \".join(review_tokens[x:y+1]))\n",
    "    return items\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print search_menu('death by chocolate', read_yelp_menu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to compute Recall and Precision given Predicted and True values\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(words)\n",
    "    filered_words = [w for w in word_tokens if not w in stop_words]\n",
    "    return \" \".join(filered_words)\n",
    "\n",
    "def preprocess(words):\n",
    "    words = remove_stop_words(words)\n",
    "    words = words.split()\n",
    "    out = []\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        word = word.lower()\n",
    "        word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "        p = nltk.PorterStemmer()\n",
    "        word = p.stem(word)\n",
    "        out.append(word)\n",
    "    out.sort()\n",
    "    out = ' '.join(out)\n",
    "    return out\n",
    "\n",
    "def custom_recall(true_y, pred_y):\n",
    "    true_y = list(set(true_y))\n",
    "    pred_y = list(set(pred_y))\n",
    "    if len(true_y) == 0:\n",
    "        return 0.0, 0.0\n",
    "    if len(pred_y) == 0:\n",
    "        return 0.0, 0.0\n",
    "    for i in range(len(true_y)):\n",
    "        curr = preprocess(true_y[i])\n",
    "        true_y[i] = curr\n",
    "    for i in range(len(pred_y)):\n",
    "        curr = preprocess(pred_y[i])\n",
    "        pred_y[i] = curr\n",
    "\n",
    "#     print('true_y: %s' % (true_y))\n",
    "#     print('pred_y: %s' % (pred_y))\n",
    "    # compute recall - fraction of true values predicted\n",
    "    count_arr = [0] * len(true_y)\n",
    "    for i in range(len(true_y)):\n",
    "        for j in range(len(pred_y)):\n",
    "          if true_y[i] in pred_y[j] or pred_y[j] in true_y[i]:\n",
    "            count_arr[i] = 1\n",
    "    recall = sum(count_arr) * 1.0 / len(true_y)\n",
    "    \n",
    "    # compute precision -  fraction of predicted values that are true\n",
    "    count_arr = [0] * len(pred_y)\n",
    "    for j in range(len(pred_y)):\n",
    "        for i in range(len(true_y)):\n",
    "          if true_y[i] in pred_y[j] or pred_y[j] in true_y[i]:\n",
    "            count_arr[j] = 1\n",
    "    precision = sum(count_arr) * 1.0 / len(pred_y)\n",
    "    return recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Main block that extracts food entities. Takes around 6 hrs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Review: #1\n",
      "Review: #2\n",
      "Review: #3\n",
      "Review: #4\n",
      "Review: #5\n",
      "Review: #6\n",
      "Review: #7\n",
      "Review: #8\n",
      "Review: #9\n",
      "Review: #10\n",
      "Review: #11\n",
      "Review: #12\n",
      "Review: #13\n",
      "Review: #14\n",
      "Review: #15\n",
      "Review: #16\n",
      "Review: #17\n",
      "Review: #18\n",
      "Review: #19\n",
      "Review: #20\n",
      "Review: #21\n",
      "Review: #22\n",
      "Review: #23\n",
      "Review: #24\n",
      "Review: #25\n",
      "Review: #26\n",
      "Review: #27\n",
      "Review: #28\n",
      "Review: #29\n",
      "Review: #30\n",
      "Review: #31\n",
      "Review: #32\n",
      "Review: #33\n",
      "Review: #34\n",
      "Review: #35\n",
      "Review: #36\n",
      "Review: #37\n",
      "Review: #38\n",
      "Review: #39\n",
      "Review: #40\n",
      "Review: #41\n",
      "Review: #42\n",
      "Review: #43\n",
      "Review: #44\n",
      "Review: #45\n",
      "Review: #46\n",
      "Review: #47\n",
      "Review: #48\n",
      "Review: #49\n",
      "Review: #50\n",
      "Review: #51\n",
      "Review: #52\n",
      "Review: #53\n",
      "Review: #54\n",
      "Review: #55\n",
      "Review: #56\n",
      "Review: #57\n",
      "Review: #58\n",
      "Review: #59\n",
      "Review: #60\n",
      "Review: #61\n",
      "Review: #62\n",
      "Review: #63\n",
      "Review: #64\n",
      "Review: #65\n",
      "Review: #66\n",
      "Review: #67\n",
      "Review: #68\n",
      "Review: #69\n",
      "Review: #70\n",
      "Review: #71\n",
      "Review: #72\n",
      "Review: #73\n",
      "Review: #74\n",
      "Review: #75\n",
      "Review: #76\n",
      "Review: #77\n",
      "Review: #78\n",
      "Review: #79\n",
      "Review: #80\n",
      "Review: #81\n",
      "Review: #82\n",
      "Review: #83\n",
      "Review: #84\n",
      "Review: #85\n",
      "Review: #86\n",
      "Review: #87\n",
      "Review: #88\n",
      "Review: #89\n",
      "Review: #90\n",
      "Review: #91\n",
      "Review: #92\n",
      "Review: #93\n",
      "Review: #94\n",
      "Review: #95\n",
      "Review: #96\n",
      "Review: #97\n",
      "Review: #98\n",
      "Review: #99\n",
      "Review: #100\n",
      "Review: #101\n",
      "Review: #102\n",
      "Review: #103\n",
      "Review: #104\n",
      "Review: #105\n",
      "Review: #106\n",
      "Review: #107\n",
      "Review: #108\n",
      "Review: #109\n",
      "Review: #110\n",
      "Review: #111\n",
      "Review: #112\n",
      "Review: #113\n",
      "Review: #114\n",
      "Review: #115\n",
      "Review: #116\n",
      "Review: #117\n",
      "Review: #118\n",
      "Review: #119\n",
      "Review: #120\n",
      "Review: #121\n",
      "Review: #122\n",
      "Review: #123\n",
      "Review: #124\n",
      "Review: #125\n",
      "Review: #126\n",
      "Review: #127\n",
      "Review: #128\n",
      "Review: #129\n",
      "Review: #130\n",
      "Review: #131\n",
      "Review: #132\n",
      "Review: #133\n",
      "Review: #134\n",
      "Review: #135\n",
      "Review: #136\n",
      "Review: #137\n",
      "Review: #138\n",
      "Review: #139\n",
      "Review: #140\n",
      "Review: #141\n",
      "Review: #142\n",
      "Review: #143\n",
      "Review: #144\n",
      "Review: #145\n",
      "Review: #146\n",
      "Review: #147\n",
      "Review: #148\n",
      "Review: #149\n",
      "Review: #150\n",
      "Review: #151\n",
      "Review: #152\n",
      "Review: #153\n",
      "Review: #154\n",
      "Review: #155\n",
      "Review: #156\n",
      "Review: #157\n",
      "Review: #158\n",
      "Review: #159\n",
      "Review: #160\n",
      "Review: #161\n",
      "Review: #162\n",
      "Review: #163\n",
      "Review: #164\n",
      "Review: #165\n",
      "Review: #166\n",
      "Review: #167\n",
      "Review: #168\n",
      "Review: #169\n",
      "Review: #170\n",
      "Review: #171\n",
      "Review: #172\n",
      "Review: #173\n",
      "Review: #174\n",
      "Review: #175\n",
      "Review: #176\n",
      "Review: #177\n",
      "Review: #178\n",
      "Review: #179\n",
      "Review: #180\n",
      "Review: #181\n",
      "Review: #182\n",
      "Review: #183\n",
      "Review: #184\n",
      "Review: #185\n",
      "Review: #186\n",
      "Review: #187\n",
      "Review: #188\n",
      "Review: #189\n",
      "Review: #190\n",
      "Review: #191\n",
      "Review: #192\n",
      "Review: #193\n",
      "Review: #194\n",
      "Review: #195\n",
      "Review: #196\n",
      "Review: #197\n",
      "Review: #198\n",
      "Review: #199\n",
      "Review: #200\n",
      "Review: #201\n",
      "Review: #202\n",
      "Review: #203\n",
      "Review: #204\n",
      "Review: #205\n",
      "Review: #206\n",
      "Review: #207\n",
      "Review: #208\n",
      "Review: #209\n",
      "Review: #210\n",
      "Review: #211\n",
      "Review: #212\n",
      "Review: #213\n",
      "Review: #214\n",
      "Review: #215\n",
      "Review: #216\n",
      "Review: #217\n",
      "Review: #218\n",
      "Review: #219\n",
      "Review: #220\n",
      "Review: #221\n",
      "Review: #222\n",
      "Review: #223\n",
      "Review: #224\n",
      "Review: #225\n",
      "Review: #226\n",
      "Review: #227\n",
      "Review: #228\n",
      "Review: #229\n",
      "Review: #230\n",
      "Review: #231\n",
      "Review: #232\n",
      "Review: #233\n",
      "Review: #234\n",
      "Review: #235\n",
      "Review: #236\n",
      "Review: #237\n",
      "Review: #238\n",
      "Review: #239\n",
      "Review: #240\n",
      "Review: #241\n",
      "Review: #242\n",
      "Review: #243\n",
      "Review: #244\n",
      "Review: #245\n",
      "Review: #246\n",
      "Review: #247\n",
      "Review: #248\n",
      "Review: #249\n",
      "Review: #250\n",
      "Review: #251\n",
      "Review: #252\n",
      "Review: #253\n",
      "Review: #254\n",
      "Review: #255\n",
      "Review: #256\n",
      "Review: #257\n",
      "Review: #258\n",
      "Review: #259\n",
      "Review: #260\n",
      "Review: #261\n",
      "Review: #262\n",
      "Review: #263\n",
      "Review: #264\n",
      "Review: #265\n",
      "Review: #266\n",
      "Review: #267\n",
      "Review: #268\n",
      "Review: #269\n",
      "Review: #270\n",
      "Review: #271\n",
      "Review: #272\n",
      "Review: #273\n",
      "Review: #274\n",
      "Review: #275\n",
      "Review: #276\n",
      "Review: #277\n",
      "Review: #278\n",
      "Review: #279\n",
      "Review: #280\n",
      "Review: #281\n",
      "Review: #282\n",
      "Review: #283\n",
      "Review: #284\n",
      "Review: #285\n",
      "Review: #286\n",
      "Review: #287\n",
      "Review: #288\n",
      "Review: #289\n",
      "Review: #290\n",
      "Review: #291\n",
      "Review: #292\n",
      "Review: #293\n",
      "Review: #294\n",
      "Review: #295\n",
      "Review: #296\n",
      "Review: #297\n",
      "Review: #298\n",
      "Review: #299\n",
      "Review: #300\n",
      "Review: #301\n",
      "Review: #302\n",
      "Review: #303\n",
      "Review: #304\n",
      "Review: #305\n",
      "Review: #306\n",
      "Review: #307\n",
      "Review: #308\n",
      "Review: #309\n",
      "Review: #310\n",
      "Review: #311\n",
      "Review: #312\n",
      "Review: #313\n",
      "Review: #314\n",
      "Review: #315\n",
      "Review: #316\n",
      "Review: #317\n",
      "Review: #318\n",
      "Review: #319\n",
      "Review: #320\n",
      "Review: #321\n",
      "Review: #322\n",
      "Review: #323\n",
      "Review: #324\n",
      "Review: #325\n",
      "Review: #326\n",
      "Review: #327\n",
      "Review: #328\n",
      "Review: #329\n",
      "Review: #330\n",
      "Review: #331\n",
      "Review: #332\n",
      "Review: #333\n",
      "Review: #334\n",
      "Review: #335\n",
      "Review: #336\n",
      "Review: #337\n",
      "Review: #338\n",
      "Review: #339\n",
      "Review: #340\n",
      "Review: #341\n",
      "Review: #342\n",
      "Review: #343\n",
      "Review: #344\n",
      "Review: #345\n",
      "Review: #346\n",
      "Review: #347\n",
      "Review: #348\n",
      "Review: #349\n",
      "Review: #350\n",
      "Review: #351\n",
      "Review: #352\n",
      "Review: #353\n",
      "Review: #354\n",
      "Review: #355\n",
      "Review: #356\n",
      "Review: #357\n",
      "Review: #358\n",
      "Review: #359\n",
      "Review: #360\n",
      "Review: #361\n",
      "Review: #362\n",
      "Review: #363\n",
      "Review: #364\n",
      "Review: #365\n",
      "Review: #366\n",
      "Review: #367\n",
      "Review: #368\n",
      "Review: #369\n",
      "Review: #370\n",
      "Review: #371\n",
      "Review: #372\n",
      "Review: #373\n",
      "Review: #374\n",
      "Review: #375\n",
      "Review: #376\n",
      "Review: #377\n",
      "Review: #378\n",
      "Review: #379\n",
      "Review: #380\n",
      "Review: #381\n",
      "Review: #382\n",
      "Review: #383\n",
      "Review: #384\n",
      "Review: #385\n",
      "Review: #386\n",
      "Review: #387\n",
      "Review: #388\n",
      "Review: #389\n",
      "Review: #390\n",
      "Review: #391\n",
      "Review: #392\n",
      "Review: #393\n",
      "Review: #394\n",
      "Review: #395\n",
      "Review: #396\n",
      "Review: #397\n",
      "Review: #398\n",
      "Review: #399\n",
      "Review: #400\n",
      "Review: #401\n",
      "Review: #402\n",
      "Review: #403\n",
      "Review: #404\n",
      "Review: #405\n",
      "Review: #406\n",
      "Review: #407\n",
      "Review: #408\n",
      "Review: #409\n",
      "Review: #410\n",
      "Review: #411\n",
      "Review: #412\n",
      "Review: #413\n",
      "Review: #414\n",
      "Review: #415\n",
      "Review: #416\n",
      "Review: #417\n",
      "Review: #418\n",
      "Review: #419\n",
      "Review: #420\n",
      "Review: #421\n",
      "Review: #422\n",
      "Review: #423\n",
      "Review: #424\n",
      "Review: #425\n",
      "Review: #426\n",
      "Review: #427\n",
      "Review: #428\n",
      "Review: #429\n",
      "Review: #430\n",
      "Review: #431\n",
      "Review: #432\n",
      "Review: #433\n",
      "Review: #434\n",
      "Review: #435\n",
      "Review: #436\n",
      "Review: #437\n",
      "Review: #438\n",
      "Review: #439\n",
      "Review: #440\n",
      "Review: #441\n",
      "Review: #442\n",
      "Review: #443\n",
      "Review: #444\n",
      "Review: #445\n",
      "Review: #446\n",
      "Review: #447\n",
      "Review: #448\n",
      "Review: #449\n",
      "Review: #450\n",
      "Review: #451\n",
      "Review: #452\n",
      "Review: #453\n",
      "Review: #454\n",
      "Review: #455\n",
      "Review: #456\n",
      "Review: #457\n",
      "Review: #458\n",
      "Review: #459\n",
      "Review: #460\n",
      "Review: #461\n",
      "Review: #462\n",
      "Review: #463\n",
      "Review: #464\n",
      "Review: #465\n",
      "Review: #466\n",
      "Review: #467\n",
      "Review: #468\n",
      "Review: #469\n",
      "Review: #470\n",
      "Review: #471\n",
      "Review: #472\n",
      "Review: #473\n",
      "Review: #474\n",
      "Review: #475\n",
      "Review: #476\n",
      "Review: #477\n",
      "Review: #478\n",
      "Review: #479\n",
      "Review: #480\n",
      "Review: #481\n",
      "Review: #482\n",
      "Review: #483\n",
      "Review: #484\n",
      "Review: #485\n",
      "Review: #486\n",
      "Review: #487\n",
      "Review: #488\n",
      "Review: #489\n",
      "Review: #490\n",
      "Review: #491\n",
      "Review: #492\n",
      "Review: #493\n",
      "Review: #494\n",
      "Review: #495\n",
      "Review: #496\n",
      "Review: #497\n",
      "Review: #498\n",
      "Review: #499\n",
      "Review: #500\n",
      "Review: #501\n",
      "Review: #502\n",
      "Review: #503\n",
      "Review: #504\n",
      "Review: #505\n",
      "Review: #506\n",
      "Review: #507\n",
      "Review: #508\n",
      "Review: #509\n",
      "Review: #510\n",
      "Review: #511\n",
      "Review: #512\n",
      "Review: #513\n",
      "Review: #514\n",
      "Review: #515\n",
      "Review: #516\n",
      "Review: #517\n",
      "Review: #518\n",
      "Review: #519\n",
      "Review: #520\n",
      "Review: #521\n",
      "Review: #522\n",
      "Review: #523\n",
      "Review: #524\n",
      "Review: #525\n",
      "Review: #526\n",
      "Review: #527\n",
      "Review: #528\n",
      "Review: #529\n",
      "Review: #530\n",
      "Review: #531\n",
      "Review: #532\n",
      "Review: #533\n",
      "Review: #534\n",
      "Review: #535\n",
      "Review: #536\n",
      "Review: #537\n",
      "Review: #538\n",
      "Review: #539\n",
      "Review: #540\n",
      "Review: #541\n",
      "Review: #542\n",
      "Review: #543\n",
      "Review: #544\n",
      "Review: #545\n",
      "Review: #546\n",
      "Review: #547\n",
      "Review: #548\n",
      "Review: #549\n",
      "Review: #550\n",
      "Review: #551\n",
      "Review: #552\n",
      "Review: #553\n",
      "Review: #554\n",
      "Review: #555\n",
      "Review: #556\n",
      "Review: #557\n",
      "Review: #558\n",
      "Review: #559\n",
      "Review: #560\n",
      "Review: #561\n",
      "Review: #562\n",
      "Review: #563\n",
      "Review: #564\n",
      "Review: #565\n",
      "Review: #566\n",
      "Review: #567\n",
      "Review: #568\n",
      "Review: #569\n",
      "Review: #570\n",
      "Review: #571\n",
      "Review: #572\n",
      "Review: #573\n",
      "Review: #574\n",
      "Review: #575\n",
      "Review: #576\n",
      "Review: #577\n",
      "Review: #578\n",
      "Review: #579\n",
      "Review: #580\n",
      "Review: #581\n",
      "Review: #582\n",
      "Review: #583\n",
      "Review: #584\n",
      "Review: #585\n",
      "Review: #586\n",
      "Review: #587\n",
      "Review: #588\n",
      "Review: #589\n",
      "Review: #590\n",
      "Review: #591\n",
      "Review: #592\n",
      "Review: #593\n",
      "Review: #594\n",
      "Review: #595\n",
      "Review: #596\n",
      "Review: #597\n",
      "Review: #598\n",
      "Review: #599\n",
      "Review: #600\n",
      "Review: #601\n",
      "Review: #602\n",
      "Review: #603\n",
      "Review: #604\n",
      "Review: #605\n",
      "Review: #606\n",
      "Review: #607\n",
      "Review: #608\n",
      "Review: #609\n",
      "Review: #610\n",
      "Review: #611\n",
      "Review: #612\n",
      "Review: #613\n",
      "Review: #614\n",
      "Review: #615\n",
      "Review: #616\n",
      "Review: #617\n",
      "Review: #618\n",
      "Review: #619\n",
      "Review: #620\n",
      "Review: #621\n",
      "Review: #622\n",
      "Review: #623\n",
      "Review: #624\n",
      "Review: #625\n",
      "Review: #626\n",
      "Review: #627\n",
      "Review: #628\n",
      "Review: #629\n",
      "Review: #630\n",
      "Review: #631\n",
      "Review: #632\n",
      "Review: #633\n",
      "Review: #634\n",
      "Review: #635\n",
      "Review: #636\n",
      "Review: #637\n",
      "Review: #638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: #639\n",
      "Review: #640\n",
      "Review: #641\n",
      "Review: #642\n",
      "Review: #643\n",
      "Review: #644\n",
      "Review: #645\n",
      "Review: #646\n",
      "Review: #647\n",
      "Review: #648\n",
      "Review: #649\n",
      "Review: #650\n",
      "Review: #651\n",
      "Review: #652\n",
      "Review: #653\n",
      "Review: #654\n",
      "Review: #655\n",
      "Review: #656\n",
      "Review: #657\n",
      "Review: #658\n",
      "Review: #659\n",
      "Review: #660\n",
      "Review: #661\n",
      "Review: #662\n",
      "Review: #663\n",
      "Review: #664\n",
      "Review: #665\n",
      "Review: #666\n",
      "Review: #667\n",
      "Review: #668\n",
      "Review: #669\n",
      "Review: #670\n",
      "Review: #671\n",
      "Review: #672\n",
      "Review: #673\n",
      "Review: #674\n",
      "Review: #675\n",
      "Review: #676\n",
      "Review: #677\n",
      "Review: #678\n",
      "Review: #679\n",
      "Review: #680\n",
      "Review: #681\n",
      "Review: #682\n",
      "Review: #683\n",
      "Review: #684\n",
      "Review: #685\n",
      "Review: #686\n",
      "Review: #687\n",
      "Review: #688\n",
      "Review: #689\n",
      "Review: #690\n",
      "Review: #691\n",
      "Review: #692\n",
      "Review: #693\n",
      "Review: #694\n",
      "Review: #695\n",
      "Review: #696\n",
      "Review: #697\n",
      "Review: #698\n",
      "Review: #699\n",
      "Review: #700\n",
      "Review: #701\n",
      "Review: #702\n",
      "Review: #703\n",
      "Review: #704\n",
      "Review: #705\n",
      "Review: #706\n",
      "Review: #707\n",
      "Review: #708\n",
      "Review: #709\n",
      "Review: #710\n",
      "Review: #711\n",
      "Review: #712\n",
      "Review: #713\n",
      "Review: #714\n",
      "Review: #715\n",
      "Review: #716\n",
      "Review: #717\n",
      "Review: #718\n",
      "Review: #719\n",
      "Review: #720\n",
      "Review: #721\n",
      "Review: #722\n",
      "Review: #723\n",
      "Review: #724\n",
      "Review: #725\n",
      "Review: #726\n",
      "Review: #727\n",
      "Review: #728\n",
      "Review: #729\n",
      "Review: #730\n",
      "Review: #731\n",
      "Review: #732\n",
      "Review: #733\n",
      "Review: #734\n",
      "Review: #735\n",
      "Review: #736\n",
      "Review: #737\n",
      "Review: #738\n",
      "Review: #739\n",
      "Review: #740\n",
      "Review: #741\n",
      "Review: #742\n",
      "Review: #743\n",
      "Review: #744\n",
      "Review: #745\n",
      "Review: #746\n",
      "Review: #747\n",
      "Review: #748\n",
      "Review: #749\n",
      "Review: #750\n",
      "Review: #751\n",
      "Review: #752\n",
      "Review: #753\n",
      "Review: #754\n",
      "Review: #755\n",
      "Review: #756\n",
      "Review: #757\n",
      "Review: #758\n",
      "Review: #759\n",
      "Review: #760\n",
      "Review: #761\n",
      "Review: #762\n",
      "Review: #763\n",
      "Review: #764\n",
      "Review: #765\n",
      "Review: #766\n",
      "Review: #767\n",
      "Review: #768\n",
      "Review: #769\n",
      "Review: #770\n",
      "Review: #771\n",
      "Review: #772\n",
      "Review: #773\n",
      "Review: #774\n",
      "Review: #775\n",
      "Review: #776\n",
      "Review: #777\n",
      "Review: #778\n",
      "Review: #779\n",
      "Review: #780\n",
      "Review: #781\n",
      "Review: #782\n",
      "Review: #783\n",
      "Review: #784\n",
      "Review: #785\n",
      "Review: #786\n",
      "Review: #787\n",
      "Review: #788\n",
      "Review: #789\n",
      "Review: #790\n",
      "Review: #791\n",
      "Review: #792\n",
      "Review: #793\n",
      "Review: #794\n",
      "Review: #795\n",
      "Review: #796\n",
      "Review: #797\n",
      "Review: #798\n",
      "Avg Recal: 468.984524 Avg Precision: 192.381953\n"
     ]
    }
   ],
   "source": [
    "# read reviews and extract food entities\n",
    "import json\n",
    "import re\n",
    "\n",
    "# read true value entities for each review\n",
    "# uncomment below to run!\n",
    "\n",
    "# review_tags = {}\n",
    "# with open('data/review_tags.json', 'r') as json_file:\n",
    "#     for line in json_file:\n",
    "#         json_line = json.loads(line)\n",
    "#         review_tags[json_line['review_id']] = json_line['entities']\n",
    "\n",
    "\n",
    "# print 'starting'\n",
    "# pred_writer = open('data/predicted_tags.json', 'w')\n",
    "# unwanted_chars_re = '[\\'\\\"\\-0-9\\\\\\\\]'\n",
    "# ptr = 0\n",
    "# with open('data/reviews.json', 'r') as json_file:\n",
    "#     lines_read = 0\n",
    "#     yelp_menu = read_yelp_menu()\n",
    "#     avg_recall = 0.0\n",
    "#     avg_precision = 0.0\n",
    "#     for line in json_file:\n",
    "#         if lines_read >= ptr:\n",
    "#             json_line = json.loads(line)\n",
    "#             review =  re.sub(unwanted_chars_re, '', json_line['text'].lower())\n",
    "#             review = review.replace('\\n', ' ')\n",
    "#             print('Review: #%d' % (lines_read+1))\n",
    "# #             print '\\nreview: %s' % (review)\n",
    "#             pred_items = segment_review_len(review, yelp_menu)\n",
    "#             true_items = [ent['entity'] for ent in review_tags[json_line['review_id']]]\n",
    "# #             print 'pred_items: %s' % (pred_items)\n",
    "# #             print 'true_items: %s' % (true_items)\n",
    "#             recall, precision = custom_recall(true_items, pred_items)\n",
    "# #             print('Recall: %f Precision: %f' % (recall, precision))\n",
    "#             avg_recall += recall\n",
    "#             avg_precision += precision\n",
    "#             pred_value = {}\n",
    "#             pred_value['review_id'] = json_line['review_id']\n",
    "#             pred_value['entities'] = list(pred_items)\n",
    "#             pred_writer.write('%s\\n' % (json.dumps(pred_value)))\n",
    "# #             y = raw_input(\"Continue to next review: (y/n)\")\n",
    "# #             if y == 'n':\n",
    "# #                 ptr = lines_read\n",
    "# #                 break\n",
    "#         lines_read = lines_read +1\n",
    "# pred_writer.close()\n",
    "# print 'Avg Recal: %f Avg Precision: %f' % (avg_recall, avg_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering extracted entities\n",
    "Above generated entities have heavy noise. This is carry forwarded from yelp menu and also the way phrase matching is implemented in the above blocks. To reduce this noise, we pass these extracted entities to nltk food corpus, [https://wordnet.princeton.edu/documentation/lexnames5wn]. This identifies noise data and is captured in file **non-entities.txt**. Later these non-entities are removed from extracted entities and written into **filtered_predicted_tags.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to merge, sort, compare two menu items like 'chicken salad' and 'salad with chicken'\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# output: {\"review_id\": {\"business_id\": \"\", \"stars\": #}}\n",
    "def get_review_restaurant_map():\n",
    "    rev_res_map = {}\n",
    "    with open('data/reviews.json', 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            json_line = json.loads(line)\n",
    "            review_info = {}\n",
    "            review_info['business_id'] = json_line['business_id']\n",
    "            review_info['stars'] = json_line['stars']\n",
    "            rev_res_map[json_line['review_id']] = review_info\n",
    "    return rev_res_map\n",
    "\n",
    "def get_true_entities(rev_res_map):\n",
    "    item_scores_true = {}\n",
    "    with open('data/review_tags.json', 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            item_scores = {}\n",
    "            json_line = json.loads(line)\n",
    "            review_id = json_line['review_id']\n",
    "            business_id = rev_res_map[review_id]['business_id']\n",
    "            if business_id in item_scores_true:\n",
    "              item_scores = item_scores_true[business_id]\n",
    "            for entity in json_line['entities']:\n",
    "              food_item = entity['entity'].lower()\n",
    "              if food_item not in item_scores:\n",
    "                item_scores[food_item] = 0\n",
    "              item_scores[food_item] += entity['sentiment']\n",
    "            item_scores_true[business_id] = item_scores\n",
    "        for res in item_scores_true:\n",
    "          item_scores_true[res] = sorted(item_scores_true[res].items(), key=lambda x: x[1], reverse=True)\n",
    "    return item_scores_true\n",
    "\n",
    "def sort_entities_map(ent_map):\n",
    "    for res in ent_map:\n",
    "          ent_map[res].sort(key=lambda x: x[1], reverse=True)\n",
    "    return ent_map\n",
    "\n",
    "# map{res_id => [entities]}\n",
    "def merge_entities(res_ent_map):\n",
    "    for res_id in res_ent_map:\n",
    "        res_ent_map[res_id] = merge_similar_items(res_ent_map[res_id])\n",
    "    return res_ent_map\n",
    "\n",
    "# items is list of tuple like ('item_name', #occurences)\n",
    "def merge_similar_items(items):\n",
    "    if len(items) == 0:\n",
    "        return items\n",
    "    i=0\n",
    "    while i < len(items):\n",
    "        j = i+1\n",
    "        while j < len(items):\n",
    "            if is_similar(items[i][0], items[j][0]):\n",
    "                # print('SIMILAR: %s <-> %s' % (items[i][0], items[j][0]))\n",
    "                items[i] = (items[i][0], items[i][1]+items[j][1])\n",
    "                del items[j]\n",
    "            else:\n",
    "                j +=1\n",
    "        i += 1\n",
    "    return items\n",
    "\n",
    "# remove filter words\n",
    "# stem each word\n",
    "def is_similar(item1, item2):\n",
    "    item1 = remove_stop_words(item1)\n",
    "    item1 = preprocess(item1)\n",
    "    item2 = remove_stop_words(item2)\n",
    "    item2 = preprocess(item2)\n",
    "    \n",
    "    # its a mismatch if items of different length\n",
    "    if len(set(item1.split())) != len(set(item2.split())):\n",
    "        return False\n",
    "    \n",
    "    found = True\n",
    "    for i1 in item1.split():\n",
    "        found = False\n",
    "        for i2 in item2.split():\n",
    "            if i1 == i2:\n",
    "                found = True\n",
    "        if not found:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(words)\n",
    "    filered_words = [w for w in word_tokens if not w in stop_words]\n",
    "    return \" \".join(filered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing Custom non-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of predicted entities\n",
    "def get_pred_entities(rev_res_map, pred_file):\n",
    "    item_scores_pred = {}\n",
    "    with open(pred_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            item_scores = {}\n",
    "            json_line = json.loads(line)\n",
    "            review_id = json_line['review_id']\n",
    "            business_id = rev_res_map[review_id]['business_id']\n",
    "            if business_id in item_scores_pred:\n",
    "              item_scores = item_scores_pred[business_id]\n",
    "            for entity in json_line['entities']:\n",
    "              food_item = entity.lower()\n",
    "              if food_item not in item_scores:\n",
    "                item_scores[food_item] = 0\n",
    "              item_scores[food_item] += 1\n",
    "            item_scores_pred[business_id] = item_scores\n",
    "        for res in item_scores_pred:\n",
    "          item_scores_pred[res] = sorted(item_scores_pred[res].items(), key=lambda x: x[1], reverse=True)\n",
    "    return item_scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res_id: Hdzo5ggPswyv-8ZlW0PVLw\n",
      "Pred: [(u'gelato', 99), (u'flavor', 27), (u'ice cream', 23), (u'chocolate', 21), (u'selection', 23), (u'cup', 21), (u'size', 12), (u'way', 11), (u'fruit', 11), (u'dinner', 9), (u'mango', 9), (u'day', 9), (u'pistachio', 8), (u'fruity', 8), (u'coffee', 7), (u'home', 7), (u'chocolate chili', 10), (u'peanut butter', 7), (u'kids', 7), (u'family', 6)]\n",
      "\n",
      "True: [(u'ferrero rocher', 8), (u'peanut butter', 9), (u'mango', 8), (u'pistachio', 7), (u'strawberry cheesecake', 6), (u'chocolate', 6), (u'grapefruit', 5), (u'grasshopper', 5), (u'lemon', 5), (u'chocolate chili', 5), (u'raspberry', 4), (u'banana', 4), (u'strawberry', 4), (u'coconut', 4), (u'gelato', 4), (u'stracciatella', 3), (u'double fudge chocolate brownie', 3), (u'death by chocolate', 3), (u'sunshine', 3), (u'pear', 3)]\n",
      "\n",
      "\n",
      "res_id: dusNIzdCaH6EoLl2hRy6cQ\n",
      "Pred: [(u'chipotle', 51), (u'strip', 28), (u'burrito', 28), (u'line', 18), (u'rice', 17), (u'meat', 13), (u'chicken', 12), (u'express', 11), (u'bowl', 11), (u'beans', 10), (u'salsa', 11), (u'order', 10), (u'lunch', 9), (u'meal', 8), (u'way', 8), (u'day', 8), (u'home', 7), (u'size', 7), (u'lettuce', 6), (u'steak', 6)]\n",
      "\n",
      "True: [(u'burrito bowl', 3), (u'burritos', 4), (u'steak burrito', 2), (u'pork', 1), (u'guacamole', 1), (u'carnitas burrito', 1), (u'chicken salad with guacamole', 1), (u'steaks', 1), (u'green tomatillo salsa', 1), (u'chicken salad', 0), (u'chicken bowl with brown rice', 1), (u'jalapeno sauce', 1), (u'chicken bowl', 1), (u'bowl', 1), (u'deep fried gluttonous', 1), (u'barbacoa', 1), (u'beef chicken salad', 1), (u'salad with chicken and steak', 1), (u'grapefruit izze', 1), (u'guacamole and chips', 1)]\n",
      "\n",
      "\n",
      "res_id: RGK23CEkDfYHWtUbRhA-bQ\n",
      "Pred: [(u'dinner', 17), (u'brunch', 17), (u'bar', 17), (u'wings', 15), (u'drinks', 20), (u'burger', 17), (u'shrimp', 14), (u'onion soup', 13), (u'order', 12), (u'shrimp burger', 12), (u'flavor', 10), (u'grits', 10), (u'side', 12), (u'salmon', 10), (u'scallops', 11), (u'fries', 9), (u'potato hash', 9), (u'wine', 9), (u'chicken', 8), (u'pork chop', 9)]\n",
      "\n",
      "True: [(u'wings', 11), (u'french onion soup', 11), (u'shrimp burger', 6), (u'scallops', 7), (u'fries', 6), (u'fried shrimp burger', 6), (u'burger', 5), (u'pork chop', 6), (u'chicken biscuit', 6), (u'oysters', 5), (u'salmon blt', 4), (u'crab cake benedict', 4), (u'salmon', 4), (u'short rib flatbread', 3), (u'beer', 3), (u'shrimp sandwich', 3), (u'wine', 3), (u'chicken', 2), (u'hanger steak', 2), (u'grits', 2)]\n",
      "\n",
      "\n",
      "res_id: ILA41PhErg4bl1royzDX4g\n",
      "Pred: [(u'dinner', 20), (u'bruschetta', 16), (u'pizza', 17), (u'wine', 13), (u'meal', 13), (u'drinks', 14), (u'bread', 12), (u'pasta', 16), (u'salad', 13), (u'chicken', 10), (u'meatballs', 10), (u'family', 9), (u'dessert', 9), (u'dish', 13), (u'water', 7), (u'date', 7), (u'lunch', 7), (u'order', 7), (u'home', 7), (u'chocolate', 6)]\n",
      "\n",
      "True: [(u'bruschetta', 11), (u'eggplant rollatini', 5), (u'meatballs', 6), (u'caesar salad', 5), (u'calamari', 4), (u'gnocchi', 3), (u'shrimp scampi', 3), (u'pasta', 3), (u'seafood risotto', 2), (u'chicken', 2), (u'margherita pizza', 2), (u'garlic bread', 2), (u'garlic knots', 2), (u'chicken marsala', 2), (u'mussels', 2), (u'beignets', 2), (u'clam linguine', 1), (u'chicken piccata', 1), (u'pesci e gamberetti with salmon', 1), (u'ravioli with a pesto sauce', 1)]\n",
      "\n",
      "\n",
      "res_id: 9UTpmQ4OhX5jNFUIu7dPPQ\n",
      "Pred: [(u'rice', 41), (u'tofu', 35), (u'soup', 34), (u'side dishes', 24), (u'meal', 20), (u'dishes', 31), (u'spicy', 16), (u'side', 22), (u'egg', 14), (u'bulgogi', 13), (u'order', 13), (u'water', 12), (u'kimchi', 11), (u'tofu soup', 13), (u'beef', 11), (u'beans', 10), (u'stone bowl', 9), (u'bowl', 9), (u'bibimbap', 8), (u'spice', 8)]\n",
      "\n",
      "True: [(u'purple rice', 16), (u'tofu soup', 12), (u'bulgogi', 11), (u'tofu', 9), (u'soon tofu', 8), (u'beef', 5), (u'bibimbap', 5), (u'pork', 4), (u'stone bowl', 4), (u'kimchi tofu', 4), (u'soup', 4), (u'seafood tofu', 3), (u'tofu stew', 3), (u'broth', 2), (u'vegetable soon tofu', 2), (u'soy beans', 2), (u'dolsotbab', 2), (u'stone pot bibimbap', 2), (u'kimchi', 2), (u'squid', 2)]\n",
      "\n",
      "\n",
      "res_id: L0aSDVHNXCl6sY4cfZQ-5Q\n",
      "Pred: [(u'thai', 47), (u'rice', 21), (u'pad thai', 16), (u'spicy', 15), (u'chicken', 14), (u'curry', 14), (u'lunch', 13), (u'soup', 15), (u'spice', 12), (u'pad', 12), (u'dish', 12), (u'flavor', 10), (u'meal', 10), (u'order', 10), (u'dinner', 8), (u'tofu', 8), (u'day', 7), (u'noodles', 8), (u'shrimp', 6), (u'vegetables', 6)]\n",
      "\n",
      "True: [(u'pad thai', 12), (u'soup', 7), (u'fried rice', 6), (u'red curry', 4), (u'pineapple curry', 3), (u'chicken pad thai', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'green curry', 2), (u'basil fried rice', 2), (u'shrimp pad thai', 2), (u'pineapple rice', 2), (u'thai tea', 2), (u'thai iced tea', 4), (u'pineapple fried rice', 2), (u'duck grapow', 2), (u'crab rangoon', 2), (u'bangkok rolls', 2), (u'double deboned stuff chicken', 1), (u'tha tea', 1)]\n",
      "\n",
      "\n",
      "res_id: FDEm-c3NAXnTVtl-hgzAhA\n",
      "Pred: [(u'fries', 74), (u'burger', 122), (u'heart', 45), (u'lard', 42), (u'grill', 39), (u'coke', 25), (u'order', 19), (u'hooters', 18), (u'fun', 16), (u'meat', 15), (u'meal', 15), (u'way', 15), (u'sugar', 13), (u'star', 13), (u'day', 13), (u'cheese', 14), (u'bar', 12), (u'water', 11), (u'beer', 13), (u'life', 11)]\n",
      "\n",
      "True: [(u'fries', 19), (u'burger', 10), (u'coke', 2), (u'shakes', 1), (u'cheeseburger', 1), (u'triple bypass', 1), (u'strawberry shake', 1), (u'quadruple bypass', 1), (u'double bypass', 1), (u'mexican sugar cola', 1), (u'hamburger', 0), (u'milkshakes', -1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "pred_entities = get_pred_entities(rev_res_map, 'data/predicted_tags.json')\n",
    "pred_entities = merge_entities(pred_entities)\n",
    "\n",
    "true_entities = get_true_entities(rev_res_map)\n",
    "true_entities = merge_entities(true_entities)\n",
    "\n",
    "for res_id in pred_entities:\n",
    "    print '\\nres_id: %s' % (res_id)\n",
    "    print 'Pred: %s\\n' % (pred_entities[res_id][:20])\n",
    "    print 'True: %s\\n' % (true_entities[res_id][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing custom list of non-entities\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# return True is atleast on word in words[]] belongs to Food.\n",
    "def is_food(words):\n",
    "    words = words.lower().split()\n",
    "    for word in words:\n",
    "        syns = wordnet.synsets(str(word), pos = wordnet.NOUN)\n",
    "        for syn in syns:\n",
    "            if 'food' in syn.lexname():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def write_custom_stop_entities(pred_entities_map, true_entities_map):\n",
    "    stop_entity_writer = open('data/non_entities.txt', 'w')\n",
    "    for res_id in pred_entities_map:\n",
    "        for pred_entity,_ in pred_entities_map[res_id]:\n",
    "            similar = False\n",
    "            for true_entity,_ in true_entities_map[res_id]:\n",
    "                if not similar and is_similar(pred_entity, true_entity):\n",
    "                    similar = True\n",
    "            if not similar and not is_food(pred_entity):\n",
    "                stop_entity_writer.write('%s\\n' % (pred_entity))\n",
    "    stop_entity_writer.close()\n",
    "\n",
    "# uncomment this to write non_entites.txt\n",
    "# write_custom_stop_entities(pred_entities, true_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to remove stop-words, non-entities from extracted entities\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import inflect\n",
    "\n",
    "inf = inflect.engine()\n",
    "\n",
    "def get_non_entities():\n",
    "    non_entities = set()\n",
    "    with open('data/non_entities.txt', 'r') as txt_file:\n",
    "        for line in txt_file:\n",
    "            non_entities.add(line.lower().strip())\n",
    "    return non_entities\n",
    "\n",
    "def remove_trailing_stop_words(words):\n",
    "    words = words.split()\n",
    "    length = len(words)\n",
    "    len_changed = True\n",
    "    while len_changed and length > 0:\n",
    "        len_changed = False\n",
    "        w = remove_stop_words(words[-1])\n",
    "        if w == \"\":\n",
    "            len_changed = True\n",
    "            length -= 1\n",
    "            del words[-1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def remove_non_entities(entities, non_entities):\n",
    "    i = 0\n",
    "    while i < len(entities):\n",
    "        # entities is list of 'names'\n",
    "        entities[i] = remove_trailing_stop_words(entities[i])\n",
    "        if not entities[i] or entities[i] == \"\":\n",
    "            del entities[i]\n",
    "        elif entities[i] in non_entities or inf.plural(entities[i]) in non_entities:\n",
    "            del entities[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return entities\n",
    "\n",
    "def remove_non_entities_pred(pred_entities_map, non_entities):\n",
    "    for res_id in pred_entities_map:\n",
    "        entities = pred_entities_map[res_id]\n",
    "        i = 0\n",
    "        while i < len(entities):\n",
    "            # entities is tuple ('name', #score)\n",
    "            if entities[i][0] in non_entities or inf.plural(entities[i][0]) in non_entities:\n",
    "                del entities[i]\n",
    "            else:\n",
    "                i += 1\n",
    "        pred_entities_map[res_id] = entities\n",
    "    return pred_entities_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res_id: Hdzo5ggPswyv-8ZlW0PVLw\n",
      "Pred: [(u'gelato', 99), (u'ice cream', 23), (u'chocolate', 21), (u'mango', 9), (u'pistachio', 8), (u'fruity', 8), (u'coffee', 7), (u'chocolate chili', 10), (u'peanut butter', 7), (u'grapefruit', 5), (u'tiramisu', 5), (u'cake', 6), (u'banana', 5), (u'strawberry', 7), (u'mint', 5), (u'cream', 5), (u'cheesecake', 5), (u'guava', 4), (u'caramel', 4), (u'sorbets', 6)]\n",
      "\n",
      "True: [(u'ferrero rocher', 8), (u'peanut butter', 9), (u'mango', 8), (u'pistachio', 7), (u'strawberry cheesecake', 6), (u'chocolate', 6), (u'grapefruit', 5), (u'grasshopper', 5), (u'lemon', 5), (u'chocolate chili', 5), (u'raspberry', 4), (u'banana', 4), (u'strawberry', 4), (u'coconut', 4), (u'gelato', 4), (u'stracciatella', 3), (u'double fudge chocolate brownie', 3), (u'death by chocolate', 3), (u'sunshine', 3), (u'pear', 3)]\n",
      "\n",
      "\n",
      "res_id: dusNIzdCaH6EoLl2hRy6cQ\n",
      "Pred: [(u'chipotle', 51), (u'burrito', 28), (u'rice', 17), (u'meat', 13), (u'chicken', 12), (u'beans', 10), (u'salsa', 11), (u'meal', 8), (u'lettuce', 6), (u'steak', 6), (u'burrito bowl', 5), (u'cheese', 5), (u'guacamole', 5), (u'ingredients', 5), (u'water', 4), (u'barbacoa', 4), (u'beer', 4), (u'peppers', 4), (u'cream', 4), (u'tacos', 4)]\n",
      "\n",
      "True: [(u'burrito bowl', 3), (u'burritos', 4), (u'steak burrito', 2), (u'pork', 1), (u'guacamole', 1), (u'carnitas burrito', 1), (u'chicken salad with guacamole', 1), (u'steaks', 1), (u'green tomatillo salsa', 1), (u'chicken salad', 0), (u'chicken bowl with brown rice', 1), (u'jalapeno sauce', 1), (u'chicken bowl', 1), (u'bowl', 1), (u'deep fried gluttonous', 1), (u'barbacoa', 1), (u'beef chicken salad', 1), (u'salad with chicken and steak', 1), (u'grapefruit izze', 1), (u'guacamole and chips', 1)]\n",
      "\n",
      "\n",
      "res_id: RGK23CEkDfYHWtUbRhA-bQ\n",
      "Pred: [(u'brunch', 17), (u'wings', 15), (u'drinks', 20), (u'burger', 17), (u'shrimp', 14), (u'onion soup', 13), (u'shrimp burger', 12), (u'grits', 10), (u'side', 12), (u'salmon', 10), (u'scallops', 11), (u'fries', 9), (u'potato hash', 9), (u'wine', 9), (u'chicken', 8), (u'pork chop', 9), (u'cheese', 8), (u'cocktail', 8), (u'salad', 10), (u'center', 6)]\n",
      "\n",
      "True: [(u'wings', 11), (u'french onion soup', 11), (u'shrimp burger', 6), (u'scallops', 7), (u'fries', 6), (u'fried shrimp burger', 6), (u'burger', 5), (u'pork chop', 6), (u'chicken biscuit', 6), (u'oysters', 5), (u'salmon blt', 4), (u'crab cake benedict', 4), (u'salmon', 4), (u'short rib flatbread', 3), (u'beer', 3), (u'shrimp sandwich', 3), (u'wine', 3), (u'chicken', 2), (u'hanger steak', 2), (u'grits', 2)]\n",
      "\n",
      "\n",
      "res_id: ILA41PhErg4bl1royzDX4g\n",
      "Pred: [(u'bruschetta', 16), (u'pizza', 17), (u'wine', 13), (u'meal', 13), (u'drinks', 14), (u'bread', 12), (u'pasta', 16), (u'salad', 13), (u'chicken', 10), (u'meatballs', 10), (u'dish', 13), (u'water', 7), (u'date', 7), (u'chocolate', 6), (u'salmon', 6), (u'entrees', 8), (u'cheese', 5), (u'marinara', 5), (u'calamari', 5), (u'plate', 5)]\n",
      "\n",
      "True: [(u'bruschetta', 11), (u'eggplant rollatini', 5), (u'meatballs', 6), (u'caesar salad', 5), (u'calamari', 4), (u'gnocchi', 3), (u'shrimp scampi', 3), (u'pasta', 3), (u'seafood risotto', 2), (u'chicken', 2), (u'margherita pizza', 2), (u'garlic bread', 2), (u'garlic knots', 2), (u'chicken marsala', 2), (u'mussels', 2), (u'beignets', 2), (u'clam linguine', 1), (u'chicken piccata', 1), (u'pesci e gamberetti with salmon', 1), (u'ravioli with a pesto sauce', 1)]\n",
      "\n",
      "\n",
      "res_id: 9UTpmQ4OhX5jNFUIu7dPPQ\n",
      "Pred: [(u'rice', 41), (u'tofu', 35), (u'soup', 34), (u'side dishes', 24), (u'meal', 20), (u'dishes', 31), (u'side', 22), (u'egg', 14), (u'bulgogi', 13), (u'water', 12), (u'kimchi', 11), (u'tofu soup', 13), (u'beef', 11), (u'beans', 10), (u'stone bowl', 9), (u'bibimbap', 8), (u'spice', 8), (u'seafood', 7), (u'pork', 6), (u'stone pot', 6)]\n",
      "\n",
      "True: [(u'purple rice', 16), (u'tofu soup', 12), (u'bulgogi', 11), (u'tofu', 9), (u'soon tofu', 8), (u'beef', 5), (u'bibimbap', 5), (u'pork', 4), (u'stone bowl', 4), (u'kimchi tofu', 4), (u'soup', 4), (u'seafood tofu', 3), (u'tofu stew', 3), (u'broth', 2), (u'vegetable soon tofu', 2), (u'soy beans', 2), (u'dolsotbab', 2), (u'stone pot bibimbap', 2), (u'kimchi', 2), (u'squid', 2)]\n",
      "\n",
      "\n",
      "res_id: L0aSDVHNXCl6sY4cfZQ-5Q\n",
      "Pred: [(u'rice', 21), (u'pad thai', 16), (u'chicken', 14), (u'curry', 14), (u'soup', 15), (u'spice', 12), (u'dish', 12), (u'meal', 10), (u'tofu', 8), (u'noodles', 8), (u'shrimp', 6), (u'vegetables', 6), (u'meat', 4), (u'thai tea', 4), (u'tea', 4), (u'sweet', 4), (u'beef', 4), (u'basil', 4), (u'pineapple', 4), (u'basil chicken', 3)]\n",
      "\n",
      "True: [(u'pad thai', 12), (u'soup', 7), (u'fried rice', 6), (u'red curry', 4), (u'pineapple curry', 3), (u'chicken pad thai', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'green curry', 2), (u'basil fried rice', 2), (u'shrimp pad thai', 2), (u'pineapple rice', 2), (u'thai tea', 2), (u'thai iced tea', 4), (u'pineapple fried rice', 2), (u'duck grapow', 2), (u'crab rangoon', 2), (u'bangkok rolls', 2), (u'double deboned stuff chicken', 1), (u'tha tea', 1)]\n",
      "\n",
      "\n",
      "res_id: FDEm-c3NAXnTVtl-hgzAhA\n",
      "Pred: [(u'fries', 74), (u'burger', 122), (u'heart', 45), (u'lard', 42), (u'coke', 25), (u'meat', 15), (u'meal', 15), (u'cheese', 14), (u'water', 11), (u'beer', 13), (u'bun', 10), (u'patties', 15), (u'beef', 10), (u'pickles', 11), (u'onions', 10), (u'condiments', 8), (u'tomatoes', 9), (u'drinks', 6), (u'hamburger', 9), (u'lettuce', 6)]\n",
      "\n",
      "True: [(u'fries', 19), (u'burger', 10), (u'coke', 2), (u'shakes', 1), (u'cheeseburger', 1), (u'triple bypass', 1), (u'strawberry shake', 1), (u'quadruple bypass', 1), (u'double bypass', 1), (u'mexican sugar cola', 1), (u'hamburger', 0), (u'milkshakes', -1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_entities = list(get_non_entities())\n",
    "pred_entities = remove_stop_entities(pred_entities, stop_entities)\n",
    "\n",
    "for res_id in pred_entities:\n",
    "    print '\\nres_id: %s' % (res_id)\n",
    "    print 'Pred: %s\\n' % (pred_entities[res_id][:20])\n",
    "    print 'True: %s\\n' % (true_entities[res_id][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### write filtered predicted(extracted) entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def filter_predicted_entities(non_entities):\n",
    "    filter_pred_tag_writer = open('data/filtered_predicted_tags.json', 'w')\n",
    "    with open('data/predicted_tags.json', 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            json_line = json.loads(line)\n",
    "            json_line['entities'] = remove_non_entities(json_line['entities'], non_entities)\n",
    "            filter_pred_tag_writer.write('%s\\n' % (json.dumps(json_line)))\n",
    "    filter_pred_tag_writer.close()\n",
    "    \n",
    "# uncomment to write filtered predicted tags\n",
    "# filter_predicted_entities(list(get_non_entities()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Compute Popular Items\n",
    "\n",
    "### Model 1: based #occurences of each extracted entity in whole review set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res_id: Hdzo5ggPswyv-8ZlW0PVLw\n",
      "Pred: [(u'gelato', 103), (u'ice cream', 23), (u'chocolate', 12), (u'peanut butter', 9), (u'chocolate chili', 9), (u'pistachio', 8), (u'waffle cone', 8), (u'fruity', 7), (u'mango', 7), (u'strawberry cheesecake', 5)]\n",
      "\n",
      "True: [(u'peanut butter', 9), (u'ferrero rocher', 8), (u'mango', 8), (u'pistachio', 7), (u'strawberry cheesecake', 6), (u'chocolate', 6), (u'grapefruit', 5), (u'grasshopper', 5), (u'lemon', 5), (u'chocolate chili', 5)]\n",
      "\n",
      "\n",
      "res_id: dusNIzdCaH6EoLl2hRy6cQ\n",
      "Pred: [(u'chipotle', 53), (u'burrito', 27), (u'rice', 15), (u'meat', 9), (u'chicken', 7), (u'beans', 7), (u'burrito bowl', 7), (u'lettuce', 6), (u'steak', 5), (u'guacamole', 4)]\n",
      "\n",
      "True: [(u'burritos', 4), (u'burrito bowl', 3), (u'steak burrito', 2), (u'pork', 1), (u'guacamole', 1), (u'carnitas burrito', 1), (u'chicken salad with guacamole', 1), (u'steaks', 1), (u'green tomatillo salsa', 1), (u'chicken bowl with brown rice', 1)]\n",
      "\n",
      "\n",
      "res_id: RGK23CEkDfYHWtUbRhA-bQ\n",
      "Pred: [(u'burger', 14), (u'wings', 12), (u'shrimp burger', 12), (u'french onion soup', 11), (u'salmon', 9), (u'pork chop', 9), (u'salad', 9), (u'fries', 8), (u'scallops', 7), (u'beers', 7)]\n",
      "\n",
      "True: [(u'wings', 11), (u'french onion soup', 11), (u'scallops', 7), (u'shrimp burger', 6), (u'fries', 6), (u'fried shrimp burger', 6), (u'pork chop', 6), (u'chicken biscuit', 6), (u'burger', 5), (u'oysters', 5)]\n",
      "\n",
      "\n",
      "res_id: ILA41PhErg4bl1royzDX4g\n",
      "Pred: [(u'pasta', 16), (u'bruschetta', 12), (u's &', 11), (u'salad', 11), (u'pizza', 10), (u'bread', 8), (u'meatballs', 6), (u'salmon', 6), (u'eggplant rollatini', 5), (u'chicken', 5)]\n",
      "\n",
      "True: [(u'bruschetta', 11), (u'meatballs', 6), (u'eggplant rollatini', 5), (u'caesar salad', 5), (u'calamari', 4), (u'gnocchi', 3), (u'shrimp scampi', 3), (u'pasta', 3), (u'seafood risotto', 2), (u'chicken', 2)]\n",
      "\n",
      "\n",
      "res_id: 9UTpmQ4OhX5jNFUIu7dPPQ\n",
      "Pred: [(u'rice', 41), (u'soup', 34), (u'side dishes', 24), (u'soon tofu', 18), (u'tofu', 14), (u'tofu soup', 13), (u'bulgogi', 10), (u'kimchi', 10), (u'stone bowl', 9), (u'egg', 9)]\n",
      "\n",
      "True: [(u'purple rice', 16), (u'tofu soup', 12), (u'bulgogi', 11), (u'tofu', 9), (u'soon tofu', 8), (u'beef', 5), (u'bibimbap', 5), (u'pork', 4), (u'stone bowl', 4), (u'kimchi tofu', 4)]\n",
      "\n",
      "\n",
      "res_id: L0aSDVHNXCl6sY4cfZQ-5Q\n",
      "Pred: [(u'pad thai', 18), (u'chicken', 8), (u'noodles', 8), (u'soup', 8), (u'red curry', 6), (u'curry', 6), (u'fried rice', 5), (u'tofu', 5), (u'spring rolls', 5), (u'thai ice tea', 5)]\n",
      "\n",
      "True: [(u'pad thai', 12), (u'soup', 7), (u'fried rice', 6), (u'red curry', 4), (u'chicken pad thai', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'thai iced tea', 4), (u'pineapple curry', 3), (u'green curry', 2)]\n",
      "\n",
      "\n",
      "res_id: FDEm-c3NAXnTVtl-hgzAhA\n",
      "Pred: [(u'burger', 129), (u'fries', 72), (u'lard', 42), (u'heart attack grill', 40), (u'coke', 17), (u'meat', 15), (u'patties', 14), (u'pickles', 10), (u'cheese', 10), (u'beer', 10)]\n",
      "\n",
      "True: [(u'fries', 19), (u'burger', 10), (u'coke', 2), (u'shakes', 1), (u'cheeseburger', 1), (u'triple bypass', 1), (u'strawberry shake', 1), (u'quadruple bypass', 1), (u'double bypass', 1), (u'mexican sugar cola', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "pred_entities = get_pred_entities(rev_res_map, 'data/filtered_predicted_tags.json')\n",
    "pred_entities = merge_entities(pred_entities)\n",
    "pred_entities = sort_entities_map(pred_entities)\n",
    "\n",
    "true_entities = get_true_entities(rev_res_map)\n",
    "true_entities = merge_entities(true_entities)\n",
    "true_entities = sort_entities_map(true_entities)\n",
    "\n",
    "for res_id in pred_entities:\n",
    "    print '\\nres_id: %s' % (res_id)\n",
    "    print 'Pred: %s\\n' % (pred_entities[res_id][:10])\n",
    "    print 'True: %s\\n' % (true_entities[res_id][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-1: Recall: 0.742857, Precision: 0.657143\n"
     ]
    }
   ],
   "source": [
    "# precision and recall for model-1\n",
    "m1_recall = 0.0\n",
    "m1_precision = 0.0\n",
    "i=0\n",
    "for res_id in pred_entities:\n",
    "    true_y = [ent for ent,_ in true_entities[res_id][:10]]\n",
    "    pred_y = [ent for ent,_ in pred_entities[res_id][:10]]\n",
    "    recall, precision = custom_recall(true_y, pred_y)\n",
    "    m1_recall += recall\n",
    "    m1_precision += precision\n",
    "    i += 1\n",
    "m1_recall /= i\n",
    "m1_precision /= i\n",
    "print 'Model-1: Recall: %f, Precision: %f' % (m1_recall, m1_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: based on #occurences of extracted entities in positive reviews (rating >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predicted tags\n",
    "def get_m2_pred_tags(rev_res_map, pred_file):\n",
    "    item_scores_pred = {}\n",
    "    with open(pred_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            item_scores = {}\n",
    "            json_line = json.loads(line)\n",
    "            review_id = json_line['review_id']\n",
    "            business_id = rev_res_map[review_id]['business_id']\n",
    "            if business_id in item_scores_pred:\n",
    "              item_scores = item_scores_pred[business_id]\n",
    "            # only for reviews with rating >=4\n",
    "            if rev_res_map[review_id]['stars'] >= 4.0:\n",
    "                for entity in json_line['entities']:\n",
    "                  food_item = entity.lower()\n",
    "                  if food_item not in item_scores:\n",
    "                    item_scores[food_item] = 0\n",
    "                  item_scores[food_item] += 1\n",
    "            item_scores_pred[business_id] = item_scores\n",
    "        for res in item_scores_pred:\n",
    "          item_scores_pred[res] = sorted(item_scores_pred[res].items(), key=lambda x: x[1], reverse=True)\n",
    "    return item_scores_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res_id: Hdzo5ggPswyv-8ZlW0PVLw\n",
      "Pred: [(u'gelato', 72), (u'ice cream', 14), (u'chocolate', 11), (u'pistachio', 7), (u'peanut butter', 6), (u'fruity', 6), (u'chocolate chili', 6), (u'grasshopper', 5), (u'mango', 5), (u'strawberry', 5)]\n",
      "\n",
      "True: [(u'peanut butter', 9), (u'ferrero rocher', 8), (u'mango', 8), (u'pistachio', 7), (u'strawberry cheesecake', 6), (u'chocolate', 6), (u'grapefruit', 5), (u'grasshopper', 5), (u'lemon', 5), (u'chocolate chili', 5)]\n",
      "\n",
      "\n",
      "res_id: dusNIzdCaH6EoLl2hRy6cQ\n",
      "Pred: [(u'chipotle', 30), (u'burrito', 15), (u'rice', 6), (u'steak', 4), (u'lettuce', 4), (u'meat', 3), (u'guacamole', 3), (u'veggie burrito', 3), (u'beans', 3), (u'burrito bowl', 2)]\n",
      "\n",
      "True: [(u'burritos', 4), (u'burrito bowl', 3), (u'steak burrito', 2), (u'pork', 1), (u'guacamole', 1), (u'carnitas burrito', 1), (u'chicken salad with guacamole', 1), (u'steaks', 1), (u'green tomatillo salsa', 1), (u'chicken bowl with brown rice', 1)]\n",
      "\n",
      "\n",
      "res_id: RGK23CEkDfYHWtUbRhA-bQ\n",
      "Pred: [(u'wings', 10), (u'shrimp burger', 10), (u'salmon', 9), (u'burger', 9), (u'french onion soup', 8), (u'pork chop', 7), (u'beers', 7), (u'sweet potato hash', 6), (u'center', 6), (u'salads', 6)]\n",
      "\n",
      "True: [(u'wings', 11), (u'french onion soup', 11), (u'scallops', 7), (u'shrimp burger', 6), (u'fries', 6), (u'fried shrimp burger', 6), (u'pork chop', 6), (u'chicken biscuit', 6), (u'burger', 5), (u'oysters', 5)]\n",
      "\n",
      "\n",
      "res_id: ILA41PhErg4bl1royzDX4g\n",
      "Pred: [(u'pasta', 11), (u'bruschetta', 10), (u's &', 9), (u'pizza', 7), (u'salad', 6), (u'bread', 5), (u'salmon', 5), (u'meatballs', 4), (u'eggplant rollatini', 4), (u'caesar salad', 4)]\n",
      "\n",
      "True: [(u'bruschetta', 11), (u'meatballs', 6), (u'eggplant rollatini', 5), (u'caesar salad', 5), (u'calamari', 4), (u'gnocchi', 3), (u'shrimp scampi', 3), (u'pasta', 3), (u'seafood risotto', 2), (u'chicken', 2)]\n",
      "\n",
      "\n",
      "res_id: 9UTpmQ4OhX5jNFUIu7dPPQ\n",
      "Pred: [(u'rice', 33), (u'soup', 29), (u'side dishes', 16), (u'soon tofu', 12), (u'tofu soup', 11), (u'bulgogi', 10), (u'tofu', 10), (u'egg', 9), (u'stone bowl', 7), (u'bibimbap', 7)]\n",
      "\n",
      "True: [(u'purple rice', 16), (u'tofu soup', 12), (u'bulgogi', 11), (u'tofu', 9), (u'soon tofu', 8), (u'beef', 5), (u'bibimbap', 5), (u'pork', 4), (u'stone bowl', 4), (u'kimchi tofu', 4)]\n",
      "\n",
      "\n",
      "res_id: L0aSDVHNXCl6sY4cfZQ-5Q\n",
      "Pred: [(u'pad thai', 11), (u'soup', 7), (u'chicken', 6), (u'red curry', 5), (u'shrimp', 4), (u'curry', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'thai iced tea', 4), (u'green curry', 3)]\n",
      "\n",
      "True: [(u'pad thai', 12), (u'soup', 7), (u'fried rice', 6), (u'red curry', 4), (u'chicken pad thai', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'thai iced tea', 4), (u'pineapple curry', 3), (u'green curry', 2)]\n",
      "\n",
      "\n",
      "res_id: FDEm-c3NAXnTVtl-hgzAhA\n",
      "Pred: [(u'burger', 43), (u'fries', 24), (u'lard', 18), (u'heart attack grill', 17), (u'coke', 10), (u'meat', 7), (u'pickles', 6), (u'patties', 6), (u'cheese', 6), (u'hamburger', 6)]\n",
      "\n",
      "True: [(u'fries', 19), (u'burger', 10), (u'coke', 2), (u'shakes', 1), (u'cheeseburger', 1), (u'triple bypass', 1), (u'strawberry shake', 1), (u'quadruple bypass', 1), (u'double bypass', 1), (u'mexican sugar cola', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "pred_entities = get_m2_pred_tags(rev_res_map, 'data/filtered_predicted_tags.json')\n",
    "pred_entities = merge_entities(pred_entities)\n",
    "pred_entities = sort_entities_map(pred_entities)\n",
    "\n",
    "true_entities = get_true_entities(rev_res_map)\n",
    "true_entities = merge_entities(true_entities)\n",
    "true_entities = sort_entities_map(true_entities)\n",
    "\n",
    "for res_id in pred_entities:\n",
    "    print '\\nres_id: %s' % (res_id)\n",
    "    print 'Pred: %s\\n' % (pred_entities[res_id][:10])\n",
    "    print 'True: %s\\n' % (true_entities[res_id][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-2: Recall: 0.742857, Precision: 0.657143\n"
     ]
    }
   ],
   "source": [
    "# precision and recall for model-2\n",
    "m2_recall = 0.0\n",
    "m2_precision = 0.0\n",
    "i=0\n",
    "for res_id in pred_entities:\n",
    "    true_y = [ent for ent,_ in true_entities[res_id][:10]]\n",
    "    pred_y = [ent for ent,_ in pred_entities[res_id][:10]]\n",
    "    recall, precision = custom_recall(true_y, pred_y)\n",
    "    m2_recall += recall\n",
    "    m2_precision += precision\n",
    "    i += 1\n",
    "m2_recall /= i\n",
    "m2_precision /= i\n",
    "print 'Model-2: Recall: %f, Precision: %f' % (m2_recall, m2_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-3: Compute Popular, by penalizing criticized entities\n",
    "Rewarding entity from positive review (rating >= 4), as well as penalizing entity from negative review (rating <=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predicted tags\n",
    "def get_m3_pred_tags(rev_res_map, pred_file):\n",
    "    item_scores_pred = {}\n",
    "    with open(pred_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            item_scores = {}\n",
    "            json_line = json.loads(line)\n",
    "            review_id = json_line['review_id']\n",
    "            business_id = rev_res_map[review_id]['business_id']\n",
    "            if business_id in item_scores_pred:\n",
    "              item_scores = item_scores_pred[business_id]\n",
    "            # only for reviews with rating >=4\n",
    "            \n",
    "            for entity in json_line['entities']:\n",
    "                food_item = entity.lower()\n",
    "                if food_item not in item_scores:\n",
    "                    item_scores[food_item] = 0\n",
    "                # rewarding entity from positive review, as well as \n",
    "                # penalizing entity from negative review \n",
    "                item_scores[food_item] += (1 if rev_res_map[review_id]['stars'] >= 4.0 else -1)\n",
    "            item_scores_pred[business_id] = item_scores\n",
    "        for res in item_scores_pred:\n",
    "          item_scores_pred[res] = sorted(item_scores_pred[res].items(), key=lambda x: x[1], reverse=True)\n",
    "    return item_scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res_id: Hdzo5ggPswyv-8ZlW0PVLw\n",
      "Pred: [(u'gelato', 41), (u'chocolate', 10), (u'pistachio', 6), (u'ice cream', 5), (u'grasshopper', 5), (u'fruity', 5), (u'strawberry', 5), (u'grapefruit', 4), (u'fudge chocolate brownie', 4), (u'guava', 3)]\n",
      "\n",
      "True: [(u'peanut butter', 9), (u'ferrero rocher', 8), (u'mango', 8), (u'pistachio', 7), (u'strawberry cheesecake', 6), (u'chocolate', 6), (u'grapefruit', 5), (u'grasshopper', 5), (u'lemon', 5), (u'chocolate chili', 5)]\n",
      "\n",
      "\n",
      "res_id: dusNIzdCaH6EoLl2hRy6cQ\n",
      "Pred: [(u'chipotle', 7), (u'steak', 3), (u'veggie burrito', 3), (u'burritos', 3), (u'chicken bowl', 2), (u'guacamole', 2), (u'lettuce', 2), (u'brown rice', 2), (u'barbacoa', 2), (u'bottled water', 2)]\n",
      "\n",
      "True: [(u'burritos', 4), (u'burrito bowl', 3), (u'steak burrito', 2), (u'pork', 1), (u'guacamole', 1), (u'carnitas burrito', 1), (u'chicken salad with guacamole', 1), (u'steaks', 1), (u'green tomatillo salsa', 1), (u'chicken bowl with brown rice', 1)]\n",
      "\n",
      "\n",
      "res_id: RGK23CEkDfYHWtUbRhA-bQ\n",
      "Pred: [(u'salmon', 9), (u'wings', 8), (u'shrimp burger', 8), (u'beers', 7), (u'sweet potato hash', 6), (u'center', 6), (u'french onion soup', 5), (u'pork chop', 5), (u'chicken biscuit', 5), (u'salmon blt', 4)]\n",
      "\n",
      "True: [(u'wings', 11), (u'french onion soup', 11), (u'scallops', 7), (u'shrimp burger', 6), (u'fries', 6), (u'fried shrimp burger', 6), (u'pork chop', 6), (u'chicken biscuit', 6), (u'burger', 5), (u'oysters', 5)]\n",
      "\n",
      "\n",
      "res_id: ILA41PhErg4bl1royzDX4g\n",
      "Pred: [(u'bruschetta', 8), (u's &', 7), (u'pasta', 6), (u'caesar salad', 4), (u'meatball sliders', 4), (u'salmon', 4), (u'garlic', 4), (u'pizza', 4), (u'eggplant rollatini', 3), (u'risotto', 3)]\n",
      "\n",
      "True: [(u'bruschetta', 11), (u'meatballs', 6), (u'eggplant rollatini', 5), (u'caesar salad', 5), (u'calamari', 4), (u'gnocchi', 3), (u'shrimp scampi', 3), (u'pasta', 3), (u'seafood risotto', 2), (u'chicken', 2)]\n",
      "\n",
      "\n",
      "res_id: 9UTpmQ4OhX5jNFUIu7dPPQ\n",
      "Pred: [(u'rice', 25), (u'soup', 24), (u'bulgogi', 10), (u'tofu soup', 9), (u'egg', 9), (u'side dishes', 8), (u'bibimbap', 6), (u'tofu', 6), (u'soon tofu', 6), (u'stone bowl', 5)]\n",
      "\n",
      "True: [(u'purple rice', 16), (u'tofu soup', 12), (u'bulgogi', 11), (u'tofu', 9), (u'soon tofu', 8), (u'beef', 5), (u'bibimbap', 5), (u'pork', 4), (u'stone bowl', 4), (u'kimchi tofu', 4)]\n",
      "\n",
      "\n",
      "res_id: L0aSDVHNXCl6sY4cfZQ-5Q\n",
      "Pred: [(u'soup', 6), (u'chicken', 4), (u'pad thai', 4), (u'red curry', 4), (u'shrimp', 4), (u'emerald rolls', 4), (u'green curry', 3), (u'pineapple curry', 3), (u'lunch specials', 3), (u'spring rolls', 3)]\n",
      "\n",
      "True: [(u'pad thai', 12), (u'soup', 7), (u'fried rice', 6), (u'red curry', 4), (u'chicken pad thai', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'thai iced tea', 4), (u'pineapple curry', 3), (u'green curry', 2)]\n",
      "\n",
      "\n",
      "res_id: FDEm-c3NAXnTVtl-hgzAhA\n",
      "Pred: [(u'coke', 3), (u'cheese', 2), (u'pickles', 2), (u'cocacola', 2), (u'french fry bar', 2), (u'center', 2), (u'hamburger', 2), (u'condiment bar', 1), (u'cane sugar', 1), (u'everything with lard', 1)]\n",
      "\n",
      "True: [(u'fries', 19), (u'burger', 10), (u'coke', 2), (u'shakes', 1), (u'cheeseburger', 1), (u'triple bypass', 1), (u'strawberry shake', 1), (u'quadruple bypass', 1), (u'double bypass', 1), (u'mexican sugar cola', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "pred_entities = get_m3_pred_tags(rev_res_map, 'data/filtered_predicted_tags.json')\n",
    "pred_entities = merge_entities(pred_entities)\n",
    "pred_entities = sort_entities_map(pred_entities)\n",
    "\n",
    "true_entities = get_true_entities(rev_res_map)\n",
    "true_entities = merge_entities(true_entities)\n",
    "true_entities = sort_entities_map(true_entities)\n",
    "\n",
    "for res_id in pred_entities:\n",
    "    print '\\nres_id: %s' % (res_id)\n",
    "    print 'Pred: %s\\n' % (pred_entities[res_id][:10])\n",
    "    print 'True: %s\\n' % (true_entities[res_id][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-3: Recall: 0.685714, Precision: 0.585714\n"
     ]
    }
   ],
   "source": [
    "# precision and recall for model-3\n",
    "m3_recall = 0.0\n",
    "m3_precision = 0.0\n",
    "i=0\n",
    "for res_id in pred_entities:\n",
    "    true_y = [ent for ent,_ in true_entities[res_id][:10]]\n",
    "    pred_y = [ent for ent,_ in pred_entities[res_id][:10]]\n",
    "    recall, precision = custom_recall(true_y, pred_y)\n",
    "    m3_recall += recall\n",
    "    m3_precision += precision\n",
    "    i += 1\n",
    "m3_recall /= i\n",
    "m3_precision /= i\n",
    "print 'Model-3: Recall: %f, Precision: %f' % (m3_recall, m3_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-4: Differentiated rewards and penalties for entities based on review rating!\n",
    "Reward entities as below: <br> rating: 5, reward: 2 <br> rating: 4, reward: 1 <br> **rating: 3, reward: 0** <br> rating: 2, reward: -1 <br> rating: 1, reward: -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predicted tags\n",
    "def get_m4_pred_tags(rev_res_map, pred_file):\n",
    "    item_scores_pred = {}\n",
    "    with open(pred_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            item_scores = {}\n",
    "            json_line = json.loads(line)\n",
    "            review_id = json_line['review_id']\n",
    "            business_id = rev_res_map[review_id]['business_id']\n",
    "            if business_id in item_scores_pred:\n",
    "              item_scores = item_scores_pred[business_id]\n",
    "            # only for reviews with rating >=4\n",
    "            \n",
    "            for entity in json_line['entities']:\n",
    "                food_item = entity.lower()\n",
    "                if food_item not in item_scores:\n",
    "                    item_scores[food_item] = 0\n",
    "                # differentiated rewards as per review rating\n",
    "                item_scores[food_item] += (rev_res_map[review_id]['stars'] - 3.0)\n",
    "            item_scores_pred[business_id] = item_scores\n",
    "        for res in item_scores_pred:\n",
    "          item_scores_pred[res] = sorted(item_scores_pred[res].items(), key=lambda x: x[1], reverse=True)\n",
    "    return item_scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res_id: Hdzo5ggPswyv-8ZlW0PVLw\n",
      "Pred: [(u'gelato', 86.0), (u'chocolate', 16.0), (u'ice cream', 13.0), (u'fruity', 9.0), (u'strawberry', 9.0), (u'pistachio', 8.0), (u'grasshopper', 8.0), (u'mango', 8.0), (u'chocolate chili', 8.0), (u'strawberry cheesecake', 7.0)]\n",
      "\n",
      "True: [(u'peanut butter', 9), (u'ferrero rocher', 8), (u'mango', 8), (u'pistachio', 7), (u'strawberry cheesecake', 6), (u'chocolate', 6), (u'grapefruit', 5), (u'grasshopper', 5), (u'lemon', 5), (u'chocolate chili', 5)]\n",
      "\n",
      "\n",
      "res_id: dusNIzdCaH6EoLl2hRy6cQ\n",
      "Pred: [(u'chipotle', 18.0), (u'burritos', 7.0), (u'lettuce', 4.0), (u'steak', 3.0), (u'bottled water', 3.0), (u'veggie burrito', 3.0), (u'guacamole and chips', 2.0), (u'beef chicken salad', 2.0), (u'margarita', 2.0), (u'fleur de', 2.0)]\n",
      "\n",
      "True: [(u'burritos', 4), (u'burrito bowl', 3), (u'steak burrito', 2), (u'pork', 1), (u'guacamole', 1), (u'carnitas burrito', 1), (u'chicken salad with guacamole', 1), (u'steaks', 1), (u'green tomatillo salsa', 1), (u'chicken bowl with brown rice', 1)]\n",
      "\n",
      "\n",
      "res_id: RGK23CEkDfYHWtUbRhA-bQ\n",
      "Pred: [(u'wings', 16.0), (u'shrimp burger', 14.0), (u'salmon', 13.0), (u'french onion soup', 11.0), (u'pork chop', 11.0), (u'beers', 11.0), (u'salmon blt', 8.0), (u'sweet potato hash', 8.0), (u'center', 8.0), (u'scallops', 8.0)]\n",
      "\n",
      "True: [(u'wings', 11), (u'french onion soup', 11), (u'scallops', 7), (u'shrimp burger', 6), (u'fries', 6), (u'fried shrimp burger', 6), (u'pork chop', 6), (u'chicken biscuit', 6), (u'burger', 5), (u'oysters', 5)]\n",
      "\n",
      "\n",
      "res_id: ILA41PhErg4bl1royzDX4g\n",
      "Pred: [(u's &', 16.0), (u'bruschetta', 14.0), (u'pasta', 13.0), (u'pizza', 8.0), (u'meatball sliders', 7.0), (u'caesar salad', 6.0), (u'maggie', 6.0), (u'penne vodka', 6.0), (u'salmon', 6.0), (u'calamari', 5.0)]\n",
      "\n",
      "True: [(u'bruschetta', 11), (u'meatballs', 6), (u'eggplant rollatini', 5), (u'caesar salad', 5), (u'calamari', 4), (u'gnocchi', 3), (u'shrimp scampi', 3), (u'pasta', 3), (u'seafood risotto', 2), (u'chicken', 2)]\n",
      "\n",
      "\n",
      "res_id: 9UTpmQ4OhX5jNFUIu7dPPQ\n",
      "Pred: [(u'rice', 47.0), (u'soup', 36.0), (u'side dishes', 17.0), (u'bulgogi', 17.0), (u'tofu soup', 15.0), (u'tofu', 15.0), (u'soon tofu', 15.0), (u'egg', 12.0), (u'bibimbap', 11.0), (u'stone bowl', 8.0)]\n",
      "\n",
      "True: [(u'purple rice', 16), (u'tofu soup', 12), (u'bulgogi', 11), (u'tofu', 9), (u'soon tofu', 8), (u'beef', 5), (u'bibimbap', 5), (u'pork', 4), (u'stone bowl', 4), (u'kimchi tofu', 4)]\n",
      "\n",
      "\n",
      "res_id: L0aSDVHNXCl6sY4cfZQ-5Q\n",
      "Pred: [(u'pad thai', 11.0), (u'soup', 10.0), (u'red curry', 9.0), (u'chicken', 8.0), (u'shrimp', 7.0), (u'spring rolls', 7.0), (u'emerald rolls', 7.0), (u'thai iced tea', 7.0), (u'taste', 6.0), (u'pineapple', 6.0)]\n",
      "\n",
      "True: [(u'pad thai', 12), (u'soup', 7), (u'fried rice', 6), (u'red curry', 4), (u'chicken pad thai', 4), (u'spring rolls', 4), (u'emerald rolls', 4), (u'thai iced tea', 4), (u'pineapple curry', 3), (u'green curry', 2)]\n",
      "\n",
      "\n",
      "res_id: FDEm-c3NAXnTVtl-hgzAhA\n",
      "Pred: [(u'coke', 5.0), (u'pickles', 4.0), (u'french fry bar', 3.0), (u'heart', 3.0), (u'fries cooked', 2.0), (u'little star', 2.0), (u'cottage cheese', 2.0), (u'sugar cola', 2.0), (u'lard sauteed bun', 2.0), (u'specials', 2.0)]\n",
      "\n",
      "True: [(u'fries', 19), (u'burger', 10), (u'coke', 2), (u'shakes', 1), (u'cheeseburger', 1), (u'triple bypass', 1), (u'strawberry shake', 1), (u'quadruple bypass', 1), (u'double bypass', 1), (u'mexican sugar cola', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "pred_entities = get_m4_pred_tags(rev_res_map, 'data/filtered_predicted_tags.json')\n",
    "pred_entities = merge_entities(pred_entities)\n",
    "pred_entities = sort_entities_map(pred_entities)\n",
    "\n",
    "true_entities = get_true_entities(rev_res_map)\n",
    "true_entities = merge_entities(true_entities)\n",
    "true_entities = sort_entities_map(true_entities)\n",
    "\n",
    "for res_id in pred_entities:\n",
    "    print '\\nres_id: %s' % (res_id)\n",
    "    print 'Pred: %s\\n' % (pred_entities[res_id][:10])\n",
    "    print 'True: %s\\n' % (true_entities[res_id][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-4: Recall: 0.657143, Precision: 0.585714\n"
     ]
    }
   ],
   "source": [
    "# precision and recall for model-4\n",
    "m4_recall = 0.0\n",
    "m4_precision = 0.0\n",
    "i=0\n",
    "for res_id in pred_entities:\n",
    "    true_y = [ent for ent,_ in true_entities[res_id][:10]]\n",
    "    pred_y = [ent for ent,_ in pred_entities[res_id][:10]]\n",
    "    recall, precision = custom_recall(true_y, pred_y)\n",
    "    m4_recall += recall\n",
    "    m4_precision += precision\n",
    "    i += 1\n",
    "m4_recall /= i\n",
    "m4_precision /= i\n",
    "print 'Model-4: Recall: %f, Precision: %f' % (m4_recall, m4_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-5: Computing Sentiment for each entity\n",
    "\n",
    "Identifies the sentence that has given entity and computes sentiment for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sentiment for each extracted entity, considering current sentence that the entity is part of.\n",
    "import json\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "\n",
    "entities_dict = {}\n",
    "with open('filtered_predicted_tags.json') as json_file:\n",
    "    for line in json_file:\n",
    "      json_line = json.loads(line)\n",
    "      entities_dict[json_line[\"review_id\"]] = json_line[\"entities\"]\n",
    "        \n",
    "annotate_file = 'pred_sent.json'\n",
    "annotate_writer = open(annotate_file, 'a+')\n",
    "\n",
    "with open('reviews.json') as json_file:\n",
    "  for line in json_file:\n",
    "      json_line = json.loads(line)\n",
    "      # print(json_line)\n",
    "      annotation = {}\n",
    "      annotation['review_id'] = json_line['review_id']\n",
    "      annotation['entities'] = []\n",
    "      # annotation['entities'].append({'entity': entity, 'sentiment': sentiment})\n",
    "      # annotate_writer.write('%s\\n' % (json.dumps(annotation)))\n",
    "      sentences = tokenize.sent_tokenize(json_line[\"text\"])\n",
    "      pred_entities = entities_dict[json_line[\"review_id\"]]\n",
    "      for entity in pred_entities:\n",
    "        entity = entity.lower()\n",
    "        score = 0\n",
    "        count = 0\n",
    "        for i in range(len(sentences)):\n",
    "          sentence = sentences[i].lower()\n",
    "          if entity in sentence:\n",
    "            if i+1<len(sentences):\n",
    "              testimonial = TextBlob(sentences[i])\n",
    "            else:\n",
    "              testimonial = TextBlob(sentences[i])\n",
    "            score += testimonial.sentiment.polarity\n",
    "            count += 1\n",
    "            # print(sentence)\n",
    "            # print(entity)\n",
    "            # print(testimonial.sentiment.polarity)\n",
    "        if count != 0:\n",
    "          annotation['entities'].append({'entity': entity, 'sentiment': score*1.0/count})\n",
    "      annotate_writer.write('%s\\n' % (json.dumps(annotation)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predicted tags\n",
    "def get_m5_pred_tags(rev_res_map, pred_file, sent_threshold):\n",
    "    item_scores_pred = {}\n",
    "    with open(pred_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            item_scores = {}\n",
    "            json_line = json.loads(line)\n",
    "            review_id = json_line['review_id']\n",
    "            business_id = rev_res_map[review_id]['business_id']\n",
    "            if business_id in item_scores_pred:\n",
    "              item_scores = item_scores_pred[business_id]\n",
    "            \n",
    "            # consider sentiment for each entity\n",
    "            # #sent > sent_threshold => +1\n",
    "            # #sent < -1 * sent_threshold => -1\n",
    "            for ent in json_line['entities']:\n",
    "                food_item = ent['entity'].lower()\n",
    "                if food_item not in item_scores:\n",
    "                    item_scores[food_item] = 0\n",
    "                if ent['sentiment'] >= sent_threshold:\n",
    "                    item_scores[food_item] += 1\n",
    "                elif ent['sentiment'] <= -1*sent_threshold:\n",
    "                    item_scores[food_item] += -1\n",
    "            item_scores_pred[business_id] = item_scores\n",
    "        for res in item_scores_pred:\n",
    "          item_scores_pred[res] = sorted(item_scores_pred[res].items(), key=lambda x: x[1], reverse=True)\n",
    "    return item_scores_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-5: sent_threshold: 0.000000, Recall: 0.642857, Precision: 0.642857\n",
      "Model-5: sent_threshold: 0.300000, Recall: 0.571429, Precision: 0.600000\n",
      "Model-5: sent_threshold: 0.400000, Recall: 0.600000, Precision: 0.571429\n",
      "Model-5: sent_threshold: 0.500000, Recall: 0.500000, Precision: 0.514286\n",
      "Model-5: sent_threshold: 0.600000, Recall: 0.428571, Precision: 0.471429\n",
      "Model-5: sent_threshold: 0.700000, Recall: 0.442857, Precision: 0.500000\n"
     ]
    }
   ],
   "source": [
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "sent_threshold = [0.0, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for threshold in sent_threshold:\n",
    "    pred_entities = get_m5_pred_tags(rev_res_map, 'data/pred_sent.json', threshold)\n",
    "    pred_entities = merge_entities(pred_entities)\n",
    "    pred_entities = sort_entities_map(pred_entities)\n",
    "\n",
    "    true_entities = get_true_entities(rev_res_map)\n",
    "    true_entities = merge_entities(true_entities)\n",
    "    true_entities = sort_entities_map(true_entities)\n",
    "\n",
    "    # for res_id in pred_entities:\n",
    "    #     print '\\nres_id: %s' % (res_id)\n",
    "    #     print 'Pred: %s\\n' % (pred_entities[res_id][:10])\n",
    "    #     print 'True: %s\\n' % (true_entities[res_id][:10])\n",
    "\n",
    "    m5_recall = 0.0\n",
    "    m5_precision = 0.0\n",
    "    i=0\n",
    "    for res_id in pred_entities:\n",
    "        true_y = [ent for ent,_ in true_entities[res_id][:10]]\n",
    "        pred_y = [ent for ent,_ in pred_entities[res_id][:10]]\n",
    "        recall, precision = custom_recall(true_y, pred_y)\n",
    "        m5_recall += recall\n",
    "        m5_precision += precision\n",
    "        i += 1\n",
    "    m5_recall /= i\n",
    "    m5_precision /= i\n",
    "    print 'Model-5: sent_threshold: %f, Recall: %f, Precision: %f' % (threshold, m5_recall, m5_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-5.b: \n",
    "\n",
    "Capture sentiment from sentence where entity appears and from followup sentence too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sentiment for each extracted entity, considering two sentences at a time\n",
    "import json\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "\n",
    "entities_dict = {}\n",
    "with open('filtered_predicted_tags.json') as json_file:\n",
    "    for line in json_file:\n",
    "      json_line = json.loads(line)\n",
    "      entities_dict[json_line[\"review_id\"]] = json_line[\"entities\"]\n",
    "        \n",
    "annotate_file = 'pred_sent.json'\n",
    "annotate_writer = open(annotate_file, 'a+')\n",
    "\n",
    "with open('reviews.json') as json_file:\n",
    "  for line in json_file:\n",
    "      json_line = json.loads(line)\n",
    "      # print(json_line)\n",
    "      annotation = {}\n",
    "      annotation['review_id'] = json_line['review_id']\n",
    "      annotation['entities'] = []\n",
    "      # annotation['entities'].append({'entity': entity, 'sentiment': sentiment})\n",
    "      # annotate_writer.write('%s\\n' % (json.dumps(annotation)))\n",
    "      sentences = tokenize.sent_tokenize(json_line[\"text\"])\n",
    "      pred_entities = entities_dict[json_line[\"review_id\"]]\n",
    "      for entity in pred_entities:\n",
    "        entity = entity.lower()\n",
    "        score = 0\n",
    "        count = 0\n",
    "        for i in range(len(sentences)):\n",
    "          sentence = sentences[i].lower()\n",
    "          if entity in sentence:\n",
    "            if i+1<len(sentences):\n",
    "              testimonial = TextBlob(sentences[i]+ sentences[i+1])\n",
    "            else:\n",
    "              testimonial = TextBlob(sentences[i])\n",
    "            score += testimonial.sentiment.polarity\n",
    "            count += 1\n",
    "            # print(sentence)\n",
    "            # print(entity)\n",
    "            # print(testimonial.sentiment.polarity)\n",
    "        if count != 0:\n",
    "          annotation['entities'].append({'entity': entity, 'sentiment': score*1.0/count})\n",
    "      annotate_writer.write('%s\\n' % (json.dumps(annotation)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-5.b: sent_threshold: 0.000000, Recall: 0.657143, Precision: 0.628571\n",
      "Model-5.b: sent_threshold: 0.300000, Recall: 0.657143, Precision: 0.671429\n",
      "Model-5.b: sent_threshold: 0.400000, Recall: 0.600000, Precision: 0.585714\n",
      "Model-5.b: sent_threshold: 0.500000, Recall: 0.542857, Precision: 0.542857\n",
      "Model-5.b: sent_threshold: 0.600000, Recall: 0.457143, Precision: 0.442857\n",
      "Model-5.b: sent_threshold: 0.700000, Recall: 0.414286, Precision: 0.371429\n"
     ]
    }
   ],
   "source": [
    "rev_res_map = get_review_restaurant_map()\n",
    "\n",
    "sent_threshold = [0.0, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for threshold in sent_threshold:\n",
    "    pred_entities = get_m5_pred_tags(rev_res_map, 'data/pred_sent_2.json', threshold)\n",
    "    pred_entities = merge_entities(pred_entities)\n",
    "    pred_entities = sort_entities_map(pred_entities)\n",
    "\n",
    "    true_entities = get_true_entities(rev_res_map)\n",
    "    true_entities = merge_entities(true_entities)\n",
    "    true_entities = sort_entities_map(true_entities)\n",
    "\n",
    "    # for res_id in pred_entities:\n",
    "    #     print '\\nres_id: %s' % (res_id)\n",
    "    #     print 'Pred: %s\\n' % (pred_entities[res_id][:10])\n",
    "    #     print 'True: %s\\n' % (true_entities[res_id][:10])\n",
    "\n",
    "    m5_recall = 0.0\n",
    "    m5_precision = 0.0\n",
    "    i=0\n",
    "    for res_id in pred_entities:\n",
    "        true_y = [ent for ent,_ in true_entities[res_id][:10]]\n",
    "        pred_y = [ent for ent,_ in pred_entities[res_id][:10]]\n",
    "        recall, precision = custom_recall(true_y, pred_y)\n",
    "        m5_recall += recall\n",
    "        m5_precision += precision\n",
    "        i += 1\n",
    "    m5_recall /= i\n",
    "    m5_precision /= i\n",
    "    print 'Model-5.b: sent_threshold: %f, Recall: %f, Precision: %f' % (threshold, m5_recall, m5_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n",
    "It is observed from below table that introducing **sentiment analysis** (Model-5.b) **increased precision**, while **Naive Count approach** (Model-1) gave **higher Recall**.\n",
    "\n",
    "|Model    | Recall|Precision|\n",
    "|---------|-------|---------|\n",
    "|Model-1  | 74.28%|    65.7%|\n",
    "|Model-2  | 74.28%|    65.7%|\n",
    "|Model-3  |  68.5%|    58.5%|\n",
    "|Model-4  |  65.7%|    58.5%|\n",
    "|Model-5  | 64.28%|   64.28%|\n",
    "|Model-5.b|  65.7%|    67.1%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
